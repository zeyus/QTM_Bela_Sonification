---
title: "Realtime Sonification of Motion Capture Data"
keywords: 
  - Sonification
  - Motion Capture
  - Realtime
  - Processing
authors:
- name: Linus Backström
  affiliation: 1
- name: Luke Ring
  affiliation: 1
affiliations:
 - name: Aarhus University
   index: 1
citation_author: Backström, L. and Ring, L.
date: "`r format(Sys.time(), '%B %d, %Y')`"
year: 2022
bibliography:
  - master.bib
  - packages.bib
biblio-title: References
biblio-style: apa
link-citations: true
# geometry: margin=2.5cm
mainfont: georgia
sansfont: calibri
fontsize: 11pt
spacing: double
output:
  bookdown::pdf_document2:
    keep_tex: yes
    latex_engine: xelatex
    citation_package: biblatex
    toc: yes
    toc_depth: 3
    number_sections: yes
    fig_caption: yes
    template: templates/latex_template.tex
csl: templates/apa7.csl
repository: https\://github.com/zeyus/QTM_Bela_Sonification
submitted: "`r format(Sys.time(), '%B %d, %Y')`"
licensepath: https\://github.com/zeyus/QTM_Bela_Sonification/blob/main/LICENSE.md
---

```{r project_variables, include=FALSE}

# MoCap
qtm_version <- "2022.2 (build 7700)"
n_tracking_cams <- 8
n_video_cams <- 1
capture_freq <- 300

# Bela
bela_version <- "0.3.8g"
bela_freq <- 44100
bela_buffer_size <- 32
bela_n_channels <- 2
```

```{r setup, include=FALSE, cache = FALSE}

# set working directory if it isn't the directory of this file
wd <- getwd()
if (basename(wd) != "docs") {
    wd <- paste0(wd, "/docs")
    setwd(wd)
}

knitr::opts_knit$set(root.dir = paste0(getwd(), wd))
knitr::opts_chunk$set(echo = TRUE)
bookdown::pdf_document2()
# this leaves all of the intermediate files in the dir if FALSE
options(tinytex.clean = TRUE)
# make verbose TRUE to help with debugging
options(tinytex.verbose = FALSE)

# set default cran mirror
r <- getOption("repos")
r["CRAN"] <- "https://mirrors.dotsrc.org/cran/" # Denmark
options(repos = r)
rm(r)

# use this function to load packages to save manual installation
use_package <- function(p) {
    if (!is.element(p, installed.packages()[, 1]))
        install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
}

use_package("bookdown")
use_package("rticles")
use_package("tidyverse")
use_package("ggprism")
# The following are used in the preprocessing pipieline
use_package("mousetrap")

# after all the packages are loaded, we can save
# a bibliography of all the packages used
# in case we need to cite them
knitr::write_bib(.packages(), "packages.bib")
```





# Abstract

adf

# `r rmarkdown::metadata$title`

Main intro goes here (why what how?)

Single dollars ($) are required for inline mathematics e.g. $f(x) = e^{\pi/x}$

Double dollars make self-standing equations:

$$\Theta(x) = \left\{\begin{array}{l}
0\textrm{ if } x < 0\cr
1\textrm{ else}
\end{array}\right.$$

For a quick reference, the following citation commands can be used:
- @zammSynchronizingMIDIWireless2019  ->  "Author et al. (2001)"
- [@walkerMUSICALSOUNDSCAPESACCESSIBLE2007] -> "(Author et al., 2001)"
- [@vinkenAuditoryCodingHuman2013; @umekSuitabilityStrainGage2017] -> "(Author1 et al., 2001; Author2 et al., 2002)"

# Literature review (background/etc)

Interpersonal coordination describes situations where two or more individuals strive to match or complement each other's actions,
and it is rooted in both the perception and anticipation of motor actions between those individuals [@gerdschmitzSoundJoinedActions2017].
While perception, anticipation and action have traditionally been viewed as separate phenomena, current theories state that they are all intrinsically connected.
A large part of embodiment research builds on the key hypothesis that merely observing an action triggers internal models of
movement that actively engage the motor system [@gerdschmitzSoundJoinedActions2017].
As a result, the motor system appears to not only assist with the planning and controlling of one's own actions,
but also serves the secondary function of perceiving and anticipating other people's actions [@gerdschmitzSoundJoinedActions2017].
This dual-purpose function of the motor system helps with achieving interpersonal coordination and can also result in spontaneous coordination,
where the actions of others may unwittingly affect one's own actions, even after being instructed to ignore them [@demosRockingBeatEffects2012].

Sonification is defined as the use of nonspeech audio to convey information.
More specifically, sonification is the transformation of data relations into perceived relations in an acoustic
signal for the purposes of facilitating communication or interpretation [@kramerSonificationReportStatus1999].
While concepts around sonification and audification were not formalized until around the year 1992 when the first 
International Conference on Auditory Display (ICAD) was held [@dubusSonificationPhysicalQuantities2011], practical examples of sonification can 
be found throughout history. Water clocks in ancient Greece and medieval China were sometimes constructed to 
produce sounds and thereby provide auditory information about the passage of time [@dubusSonificationPhysicalQuantities2011]. 
The stethoscope, which is used for listening to sounds made by the heart and lungs as well as other internal 
sounds of the body, was invented in 1816 by French physician and amateur musician Rene Laënnec [@roguinReneTheophileHyacinthe2006]. 
The Geiger counter developed in 1928 provides perhaps the most characteristic example of sonification through 
its function of sonifying levels of radiation. The device detects ionizing radiation and translates it into audible clicks, 
where a faster tempo signifies a higher level of radiation. @dubusSonificationPhysicalQuantities2011 describe the value of the Geiger 
counter as "transposing a physical quantity which is essentially non-visual and pictured in everyone's imagination 
as very important because life-threatening, to the auditory modality through clicks with a varying pulse".


....

~~RQ: To what extent does real-time sonification of object movement effect joint action synchronization between subjects?~~

~~H_0: Real-time sonification of object movement has no effect on joint action synchronization between subjects.~~

~~H_1: Real-time sonification of object movement effects joint action synchronization between subjects.~~

now RQ is about orientation of sonification scheme (task vs sync)


# Methods

Describe subject demographics etc.

## Hardware and Software

### Motion Capture

Motion capture data were collected using a `r n_tracking_cams + n_video_cams` (`r n_tracking_cams` Qualisys Miqus M3 marker and `r n_video_cams` Qualisys Miqus Video) camera system connected to a Qualisys Camera Sync Unit.
Marker data were acquired at at a sampling rate of `r capture_freq` Hz and video data were acquired at a sampling rate of 25 Hz. Qualisys Track Manager software was used to collect and process the data with real-time 3D tracking data output.

#### Markers

For the experimental set up, one passive marker was placed on each car, and additional passive markers were placed at the start and end of each track, as well as one passive marker on each corner of the surface the tracke was mounted on (see Fig X).
These additional markers provided reference points for 3D orientation of the track and the cars across trials in case of accidental table movement.

### Sonification

#### Hardware

Motion capture data were sent via UDP packets over USB networking to a Bela Mini computer running version `r bela_version` running a custom C++ program[^code]. The main program loop 
was configured to execute every `r bela_buffer_size` samples, with an output sample rate of `r bela_freq` Hz for `r bela_n_channels` audio channels. The stereo audio output was split into
two channels which were connected to a pair of headphones for each subject.

[^code]: Source, data and analysis are available at `r rmarkdown::metadata$repository`

#### Software

The main program used the Bela platform from 10 August 2022[^belaplatform].

[^belaplatform]: https://github.com/BelaPlatform/Bela/commit/42bbf18c3710ed82cdf24b09ab72ac2239bf148e

## Experimental Design

# Analysis

# Results

# Discussion

\newpage

# References

<div id="refs"></div>