@book{almeidaDevelopmentWearableSensor2012,
  title = {Development of a Wearable Sensor System for Real-Time Control of Knee Prostheses},
  author = {de Almeida, Eduardo Carlos Venancio},
  date = {2012},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-81244},
  urldate = {2022-09-21},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\WCDV26XV\\Almeida - 2012 - Development of a wearable sensor system for real-t.pdf;C\:\\Users\\webma\\Zotero\\storage\\KIPBEW4J\\record.html}
}

@article{boltSensoryAttenuationAuditory2021,
  title = {Sensory {{Attenuation}} of the {{Auditory P2 Differentiates Self-}} from {{Partner-Produced Sounds}} during {{Joint Action}}},
  author = {Bolt, Nicole K. and Loehr, Janeen D.},
  date = {2021-10-01},
  journaltitle = {Journal of Cognitive Neuroscience},
  volume = {33},
  number = {11},
  pages = {2297--2310},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_01760},
  url = {https://direct.mit.edu/jocn/article/33/11/2297/102999/Sensory-Attenuation-of-the-Auditory-P2},
  urldate = {2022-12-02},
  abstract = {Abstract             Successful human interaction relies on people's ability to differentiate between the sensory consequences of their own and others' actions. Research in solo action contexts has identified sensory attenuation, that is, the selective perceptual or neural dampening of the sensory consequences of self-produced actions, as a potential marker of the distinction between self- and externally produced sensory consequences. However, very little research has examined whether sensory attenuation distinguishes self- from partner-produced sensory consequences in joint action contexts. The current study examined whether sensory attenuation of the auditory N1 or P2 ERPs distinguishes self- from partner-produced tones when pairs of people coordinate their actions to produce tone sequences that match a metronome pace. We did not find evidence of auditory N1 attenuation for either self- or partner-produced tones. Instead, the auditory P2 was attenuated for self-produced tones compared to partner-produced tones within the joint action. These findings indicate that self-specific attenuation of the auditory P2 differentiates the sensory consequences of one's own from others' actions during joint action. These findings also corroborate recent evidence that N1 attenuation may be driven by general rather than action-specific processes and support a recently proposed functional dissociation between auditory N1 and P2 attenuation.},
  langid = {english},
  keywords = {extracted,introduction,linus,main-paper},
  file = {C\:\\Users\\webma\\Zotero\\storage\\I32PJGPZ\\Bolt and Loehr - 2021 - Sensory Attenuation of the Auditory P2 Differentia.pdf}
}

@article{botvinickConflictMonitoringCognitive2001,
  title = {Conflict Monitoring and Cognitive Control.},
  author = {Botvinick, Matthew M. and Braver, Todd S. and Barch, Deanna M. and Carter, Cameron S. and Cohen, Jonathan D.},
  date = {2001},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {108},
  number = {3},
  pages = {624--652},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.108.3.624},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.108.3.624},
  urldate = {2022-12-19},
  langid = {english},
  keywords = {extracted}
}

@article{brockIfMotionSounds2012,
  title = {If Motion Sounds: {{Movement}} Sonification Based on Inertial Sensor Data},
  shorttitle = {If Motion Sounds},
  author = {Brock, Heike and Schmitz, Gerd and Baumann, Jan and Effenberg, Alfred O.},
  date = {2012},
  journaltitle = {Procedia Engineering},
  shortjournal = {Procedia Engineering},
  volume = {34},
  pages = {556--561},
  issn = {18777058},
  doi = {10.1016/j.proeng.2012.04.095},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877705812017080},
  urldate = {2022-09-01},
  abstract = {Within last years, movement sonification turned out to be an appropriate support for motor perception and motor control that can display physical motion in a very rich and direct way. But how should movement sonification be configured to support motor learning? The appropriate selection of movement parameters and their transformation into characteristic motion features is essential for an auditory display to become effective. In this paper, we introduce a real-time sonification framework for all common MIDI environments based on acceleration and orientation data from inertial sensors. Fundamental processing steps to transform motion information into meaningful sound will be discussed. The proposed framework of inertial motion capturing, kinematic parameter selection and possible kinematic acoustic mapping provides a basis for mobile real-time movement sonification which is a prospective powerful training tool for rehabilitation and sports and offers a broad variety of application possibilities.},
  langid = {english},
  keywords = {linus,luke,methods},
  file = {C\:\\Users\\webma\\Zotero\\storage\\HYCVDNZ8\\Brock et al. - 2012 - If motion sounds Movement sonification based on i.pdf}
}

@article{demosRockingBeatEffects2012,
  title = {Rocking to the Beat: {{Effects}} of Music and Partner's Movements on Spontaneous Interpersonal Coordination.},
  shorttitle = {Rocking to the Beat},
  author = {Demos, Alexander P. and Chaffin, Roger and Begosh, Kristen T. and Daniels, Jennifer R. and Marsh, Kerry L.},
  date = {2012-02},
  journaltitle = {Journal of Experimental Psychology: General},
  shortjournal = {Journal of Experimental Psychology: General},
  volume = {141},
  number = {1},
  pages = {49--53},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0023843},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0023843},
  urldate = {2022-10-15},
  langid = {english},
  keywords = {introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\HN8UYBSS\\Demos et al. - 2012 - Rocking to the beat Effects of music and partner'.pdf}
}

@incollection{dixLatencyCyberPhysicalSystems2022,
  title = {Latency in {{Cyber-Physical Systems}}: {{The Role}} of {{Visual Feedback Delays}} on {{Manual Skill Learning}}},
  shorttitle = {Latency in {{Cyber-Physical Systems}}},
  author = {Dix, Annika and Helmert, Jens and Pannasch, Sebastian},
  date = {2022-01-01},
  pages = {1138--1146},
  doi = {10.1007/978-3-030-85540-6_146},
  abstract = {To inform, guide and optimize the design of cyber-physical systems (CPS), this study examined whether and how delayed visual feedback affects human fine-motor skills. Two experiments are presented, in which participants performed a complex motor task with their hands. During the task, visual feedback was provided on a display with varying delay lengths. Further, to investigate effects of adaptation and transfer, some participants were first exposed to a fixed delay before performing the task with varying delay lengths. Results show that independent of earlier delay exposure feedback delays had detrimental effects on performance, particularly when performance information was lacking and delay length variable. Hand kinematic indicate the use of a strategy geared to a slowing of the movement process. Implications and future research ideas like the application of augmented feedback on movement kinematic and the examination of different settings to promote adaptation effects are discussed.},
  isbn = {978-3-030-85539-0},
  keywords = {latency}
}

@article{dubusInteractiveSonificationMotion2013,
  title = {Interactive Sonification of Motion : {{Design}}, Implementation and Control of Expressive Auditory Feedback with Mobile Devices},
  shorttitle = {Interactive Sonification of Motion},
  author = {Dubus, GaÃ«l},
  date = {2013},
  publisher = {{KTH Royal Institute of Technology}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-127944},
  urldate = {2022-09-05},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\V4JITEWG\\Dubus - 2013 - Interactive sonification of motion  Design, imple.pdf;C\:\\Users\\webma\\Zotero\\storage\\94UMW8VZ\\record.html}
}

@article{dubusSonificationPhysicalQuantities2011,
  title = {Sonification of {{Physical Quantities Throughout History}}: {{A Meta-Study}} of {{Previous Mapping Strategies}}},
  author = {Dubus, Gael and Bresin, Roberto},
  date = {2011-06},
  journaltitle = {International Conference on Auditory Display, 2011},
  pages = {8},
  abstract = {We introduce a meta-study of previous sonification designs taking physical quantities as input data. The aim is to build a solid foundation for future sonification works so that auditory display researchers would be able to take benefit from former studies, avoiding to start from scratch when beginning new sonification projects. This work is at an early stage and the objective of this paper is rather to introduce the methodology than to come to definitive conclusions. After a historical introduction, we explain how to collect a large amount of articles and extract useful information about mapping strategies. Then, we present the physical quantities grouped according to conceptual dimensions, as well as the sound parameters used in sonification designs and we summarize the current state of the study by listing the couplings extracted from the article database. A total of 54 articles have been examined for the present article. Finally, a preliminary analysis of the results is performed.},
  langid = {english},
  keywords = {introduction,linus,luke,useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\R5PXJFRK\\Dubus and Bresin - 2011 - SONIFICATION OF PHYSICAL QUANTITIES THROUGHOUT HIS.pdf}
}

@article{dubusSystematicReviewMapping2013,
  title = {A {{Systematic Review}} of {{Mapping Strategies}} for the {{Sonification}} of {{Physical Quantities}}},
  author = {Dubus, GaÃ«l and Bresin, Roberto},
  date = {2013-12-17},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  number = {12},
  pages = {e82491},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0082491},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0082491},
  urldate = {2022-09-05},
  abstract = {The field of sonification has progressed greatly over the past twenty years and currently constitutes an established area of research. This article aims at exploiting and organizing the knowledge accumulated in previous experimental studies to build a foundation for future sonification works. A systematic review of these studies may reveal trends in sonification design, and therefore support the development of design guidelines. To this end, we have reviewed and analyzed 179 scientific publications related to sonification of physical quantities. Using a bottom-up approach, we set up a list of conceptual dimensions belonging to both physical and auditory domains. Mappings used in the reviewed works were identified, forming a database of 495 entries. Frequency of use was analyzed among these conceptual dimensions as well as higher-level categories. Results confirm two hypotheses formulated in a preliminary study: pitch is by far the most used auditory dimension in sonification applications, and spatial auditory dimensions are almost exclusively used to sonify kinematic quantities. To detect successful as well as unsuccessful sonification strategies, assessment of mapping efficiency conducted in the reviewed works was considered. Results show that a proper evaluation of sonification mappings is performed only in a marginal proportion of publications. Additional aspects of the publication database were investigated: historical distribution of sonification works is presented, projects are classified according to their primary function, and the sonic material used in the auditory display is discussed. Finally, a mapping-based approach for characterizing sonification is proposed.},
  langid = {english},
  keywords = {Drug synthesis,Kinematics,Physical mapping,Pitch perception,Sensory perception,Signal filtering,Sonification,Systematic reviews},
  file = {C\:\\Users\\webma\\Zotero\\storage\\ILLLBKVQ\\Dubus and Bresin - 2013 - A Systematic Review of Mapping Strategies for the .pdf;C\:\\Users\\webma\\Zotero\\storage\\3DTNDWBP\\article.html}
}

@article{effenbergAccelerationDecelerationConstant2018,
  title = {Acceleration and Deceleration at Constant Speed: Systematic Modulation of Motion Perception by Kinematic Sonification},
  shorttitle = {Acceleration and Deceleration at Constant Speed},
  author = {Effenberg, Alfred O. and Schmitz, Gerd},
  date = {2018-08},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann. N.Y. Acad. Sci.},
  volume = {1425},
  number = {1},
  pages = {52--69},
  issn = {0077-8923, 1749-6632},
  doi = {10.1111/nyas.13693},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/nyas.13693},
  urldate = {2022-09-01},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\QCZDNVFK\\Effenberg and Schmitz - 2018 - Acceleration and deceleration at constant speed s.pdf}
}

@incollection{gerdschmitzSoundJoinedActions2017,
  title = {Sound {{Joined Actions}} in {{Rowing}} and {{Swimming}}},
  booktitle = {Moving Bodies in Interaction-- Interacting Bodies in Motion: Intercorporeality, Interkinaesthesia, and Enaction in Sports},
  author = {{Gerd Schmitz} and {Alfred O. Effenberg}},
  editor = {Meyer, Christian and Wedelstaedt, Ulrich V.},
  date = {2017},
  series = {Advances in Interaction Studies},
  number = {volume 8},
  pages = {193--214},
  publisher = {{John Benjamins Publishing Company}},
  location = {{Amsterdam ; Philadelphia}},
  abstract = {The present chapter introduces the method of sonification as a tool for studying intercorporeality and enactment. We show that auditory movement information can support motor perception as well as the control of movements, and explain these effects by mechanisms which are consistent with the enactment approach. Providing additional auditory information about a movement enables the acting individual as well as observers to perceive the movement in exactly the same way via audition. Thus, a sonification can establish a common percept for all interaction partners, which corresponds well to the concept of intercorporeality. Furthermore, we show that sonifications can be specifically designed to constitute a variety of frameworks for the analysis of interpersonal coordination and intercorporeality.},
  isbn = {978-90-272-6555-5},
  keywords = {Athletes,Discourse analysis,highlighted,Interaction (Philosophy),introduction,Kinesiology,linus,Movement; Psychology of,Physiological aspects,Physiology,Sports},
  file = {C\:\\Users\\webma\\Zotero\\storage\\SJNFT4JV\\Schmitz_Effenberg_Sound_Joined_Actions_final.pdf}
}

@article{hwangEffectPerformanceBasedAuditory2018,
  title = {Effect- and {{Performance-Based Auditory Feedback}} on {{Interpersonal Coordination}}},
  author = {Hwang, Tong-Hun and Schmitz, Gerd and Klemmt, Kevin and Brinkop, Lukas and Ghai, Shashank and Stoica, Mircea and Maye, Alexander and Blume, Holger and Effenberg, Alfred O.},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00404},
  urldate = {2022-09-01},
  abstract = {When two individuals interact in a collaborative task, such as carrying a sofa or a table, usually spatiotemporal coordination of individual motor behavior will emerge. In many cases, interpersonal coordination can arise independently of verbal communication, based on the observation of the partners' movements and/or the object's movements. In this study, we investigate how social coupling between two individuals can emerge in a collaborative task under different modes of perceptual information. A visual reference condition was compared with three different conditions with new types of additional auditory feedback provided in real time: effect-based auditory feedback, performance-based auditory feedback, and combined effect/performance-based auditory feedback. We have developed a new paradigm in which the actions of both participants continuously result in a seamlessly merged effect on an object simulated by a tablet computer application. Here, participants should temporally synchronize their movements with a 90Â° phase difference and precisely adjust the finger dynamics in order to keep the object (a ball) accurately rotating on a given circular trajectory on the tablet. Results demonstrate that interpersonal coordination in a joint task can be altered by different kinds of additional auditory information in various ways.},
  keywords = {useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\IQ3HXJFR\\Hwang et al. - 2018 - Effect- and Performance-Based Auditory Feedback on.pdf}
}

@article{kellerRhythmJointAction2014,
  title = {Rhythm in Joint Action: Psychological and Neurophysiological Mechanisms for Real-Time Interpersonal Coordination},
  shorttitle = {Rhythm in Joint Action},
  author = {Keller, Peter E. and Novembre, Giacomo and Hove, Michael J.},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1658},
  pages = {20130394},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2013.0394},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2013.0394},
  urldate = {2022-12-14},
  abstract = {Human interaction often requires simultaneous precision and flexibility in the coordination of rhythmic behaviour between individuals engaged in joint activity, for example, playing a musical duet or dancing with a partner. This review article addresses the psychological processes and brain mechanisms that enable such rhythmic interpersonal coordination. First, an overview is given of research on the cognitive-motor processes that enable individuals to represent joint action goals and to anticipate, attend and adapt to other's actions in real time. Second, the neurophysiological mechanisms that underpin rhythmic interpersonal coordination are sought in studies of sensorimotor and cognitive processes that play a role in the representation and integration of self- and other-related actions within and between individuals' brains. Finally, relationships between socialâpsychological factors and rhythmic interpersonal coordination are considered from two perspectives, one concerning how social-cognitive tendencies (e.g. empathy) affect coordination, and the other concerning how coordination affects interpersonal affiliation, trust and prosocial behaviour. Our review highlights musical ensemble performance as an ecologically valid yet readily controlled domain for investigating rhythm in joint action.},
  keywords = {interpersonal coordination,joint action,linus,musical ensembles,rhythm,sensorimotor synchronization,social neuroscience},
  file = {C\:\\Users\\webma\\Zotero\\storage\\PALZG7J6\\Keller et al_2014_Rhythm in joint action.pdf}
}

@incollection{knoblichPsychologicalResearchJoint2011,
  title = {Psychological {{Research}} on {{Joint Action}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Knoblich, GÃ¼nther and Butterfill, Stephen and Sebanz, Natalie},
  date = {2011},
  volume = {54},
  pages = {59--101},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-385527-5.00003-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123855275000036},
  urldate = {2022-12-18},
  isbn = {978-0-12-385527-5},
  langid = {english},
  keywords = {extracted,linus}
}

@inproceedings{kosBiofeedbackSportChallenges2015,
  title = {Biofeedback in Sport: {{Challenges}} in Real-Time Motion Tracking and Processing},
  shorttitle = {Biofeedback in Sport},
  booktitle = {2015 {{IEEE}} 15th {{International Conference}} on {{Bioinformatics}} and {{Bioengineering}} ({{BIBE}})},
  author = {Kos, Anton and Umek, Anton and Tomazic, Saso},
  date = {2015-11},
  pages = {1--4},
  doi = {10.1109/BIBE.2015.7367681},
  abstract = {Science and technology are ever more frequently used in sports for achieving the competitive advantage. Motion tracking systems, in connection to the biomechanical biofeedback, help in accelerating motor learning. Requirements about various parameters important in real-time biofeedback applications are discussed. Special focus is given on feedback loop delays and its real-time operation. Optical tracking and inertial sensor tracking systems are presented and compared. Real-time sensor signal acquisitions and real-time processing challenges, in connection to biomechanical biofeedback, are presented. This paper can serve as a starting point for determining the adequate combination of technical equipment and its specifications that work favorably for the operation of the planned real-time biofeedback application.},
  eventtitle = {2015 {{IEEE}} 15th {{International Conference}} on {{Bioinformatics}} and {{Bioengineering}} ({{BIBE}})},
  keywords = {Biological control systems,Biomedical optical imaging,Delays,Feedback loop,Gyroscopes,introduction,latency,Real-time systems,Tracking},
  file = {C\:\\Users\\webma\\Zotero\\storage\\PYVTBGBM\\Kos et al. - 2015 - Biofeedback in sport Challenges in real-time moti.pdf;C\:\\Users\\webma\\Zotero\\storage\\TVQCQ34F\\7367681.html}
}

@article{kourtisAttentionAllocationTask2014,
  title = {Attention {{Allocation}} and {{Task Representation}} during {{Joint Action Planning}}},
  author = {Kourtis, Dimitrios and Knoblich, GÃ¼nther and WoÅºniak, Mateusz and Sebanz, Natalie},
  date = {2014-10-01},
  journaltitle = {Journal of Cognitive Neuroscience},
  volume = {26},
  number = {10},
  pages = {2275--2286},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00634},
  url = {https://direct.mit.edu/jocn/article/26/10/2275/28187/Attention-Allocation-and-Task-Representation},
  urldate = {2022-12-14},
  abstract = {Abstract             We investigated whether people take into account an interaction partner's attentional focus and whether they represent in advance their partner's part of the task when planning to engage in a synchronous joint action. The experiment involved two participants planning and performing joint actions (i.e., synchronously lifting and clinking glasses), unimanual individual actions (i.e., lifting and moving a glass as if clinking with another person), and bimanual individual actions. EEG was recorded from one of the participants. We employed a choice reaction paradigm where a visual cue indicated the type of action to be planned, followed 1.5 sec later by a visual go stimulus, prompting the participants to act. We studied attention allocation processes by examining two lateralized EEG components, namely the anterior directing attention negativity and the late directing attention positivity. Action planning processes were examined using the late contingent negative variation and the movement-related potential. The results show that early stages of joint action planning involve dividing attention between locations in space relevant for one's own part of the joint action and locations relevant for one's partner's part of the joint action. At later stages of joint action planning, participants represented in advance their partner's upcoming action in addition to their own action, although not at an effector-specific level. Our study provides electrophysiological evidence supporting the operation of attention sharing processes and predictive self/other action representation during the planning phase of a synchronous joint task.},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\LEAUPDEW\\Kourtis et al. - 2014 - Attention Allocation and Task Representation durin.pdf}
}

@article{kourtisPredictiveRepresentationOther2012,
  title = {Predictive Representation of Other People's Actions in Joint Action Planning: {{An EEG}} Study},
  shorttitle = {Predictive Representation of Other People's Actions in Joint Action Planning},
  author = {Kourtis, Dimitrios and Sebanz, N and Knoblich, G},
  date = {2012-06-06},
  journaltitle = {Social neuroscience},
  shortjournal = {Social neuroscience},
  volume = {8},
  doi = {10.1080/17470919.2012.694823},
  abstract = {It has been postulated that when people engage in joint actions they form internal representations not only of their part of the joint task but of their co-actors' parts of the task as well. However, empirical evidence for this claim is scarce. By means of high-density electroencephalography, this study investigated whether one represents and simulates the action of an interaction partner when planning to perform a joint action. The results showed that joint action planning compared with individual action planning resulted in amplitude modulations of the frontal P3a and parietal P3b event-related potentials, which are associated with stimulus classification, updating of representations, and decision-making. Moreover, there was evidence for anticipatory motor simulation of the partner's action in the amplitude and peak latency of the late, motor part of the Contingent Negative Variation, which was correlated with joint action performance. Our results provide evidence that when people engage in joint tasks, they represent in advance each other's actions in order to facilitate coordination.},
  keywords = {extracted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\7JHZ2WQC\\Kourtis et al. - 2012 - Predictive representation of other people's action.pdf}
}

@misc{kramerSonificationReportStatus1999,
  title = {Sonification {{Report}}: {{Status}} of the {{Field}} and {{Research Agenda}}},
  author = {Kramer, Gregory and Walker, Bruce and Bonebright, Terri and Cook, Perry},
  date = {1999},
  publisher = {{International Community for Auditory Display}},
  keywords = {highlighted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\RDTBT4TR\\Kramer et al. - 1999 - Sonification Report Status of the Field and Resea.pdf}
}

@article{krascekWebBasedElearning2015,
  title = {Web {{Based E-learning Tool}} for {{Visualization}} and {{Analysis}} of {{3D Motion Capture Data}}},
  author = {KraÅ¡Äek, AndraÅ¾ and Stojmenova, Kristina and TomaÅ¾iÄ, SaÅ¡o and Sodnik, Jaka},
  date = {2015},
  journaltitle = {ACHI 2015},
  pages = {143},
  abstract = {AbstractâIn this paper, we propose an e-learning tool for visualization and manipulation of 3D data on a web platform. The data is streamed in real time from an optical motion capture system Qualisys consisting of eight infrared cameras and Qualisys Track Manager (QTM) software. A WebSocket protocol and WebGL application programming interface (API) are used to visualize and to interact with the data in a browser. The tool represents a web-based extension of QTM software providing also additional features and new possibilities to manipulate and analyze the data. We report also on a user study in which we evaluated the web based application and compared it with the original desktop-based application. The proposed application proved to be fast, effective and intuitive and can be used as an e-learning tool for demonstrating and teaching techniques for visualization and analysis of motion capture data. Keywords-motion capture; Qualisys; e-learning; 3D data; AIM model; WebGL; WebSocket. I.},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\4JMEH4XF\\KraÅ¡Äek et al. - Web Based E-learning Tool for Visualization and An.pdf;C\:\\Users\\webma\\Zotero\\storage\\5QD5TBKB\\download.html}
}

@article{loehrMonitoringIndividualJoint2013,
  title = {Monitoring {{Individual}} and {{Joint Action Outcomes}} in {{Duet Music Performance}}},
  author = {Loehr, Janeen D. and Kourtis, Dimitrios and Vesper, Cordula and Sebanz, Natalie and Knoblich, GÃ¼nther},
  date = {2013-07-01},
  journaltitle = {Journal of Cognitive Neuroscience},
  volume = {25},
  number = {7},
  pages = {1049--1061},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00388},
  url = {https://direct.mit.edu/jocn/article/25/7/1049/27943/Monitoring-Individual-and-Joint-Action-Outcomes-in},
  urldate = {2022-12-02},
  abstract = {Abstract             We investigated whether people monitor the outcomes of their own and their partners' individual actions as well as the outcome of their combined actions when performing joint actions together. Pairs of pianists memorized both parts of a piano duet. Each pianist then performed one part while their partner performed the other; EEG was recorded from both. Auditory outcomes (pitches) associated with keystrokes produced by the pianists were occasionally altered in a way that either did or did not affect the joint auditory outcome (i.e., the harmony of a chord produced by the two pianists' combined pitches). Altered auditory outcomes elicited a feedback-related negativity whether they occurred in the pianist's own part or the partner's part, and whether they affected individual or joint action outcomes. Altered auditory outcomes also elicited a P300 whose amplitude was larger when the alteration affected the joint outcome compared with individual outcomes and when the alteration affected the pianist's own part compared with the partner's part. Thus, musicians engaged in joint actions monitor their own and their partner's actions as well as their combined action outcomes, while at the same time maintaining a distinction between their own and others' actions and between individual and joint outcomes.},
  langid = {english},
  keywords = {extracted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\8KJ3IZXM\\Loehr et al. - 2013 - Monitoring Individual and Joint Action Outcomes in.pdf}
}

@article{loehrSoundYouMe2016,
  title = {The Sound of You and Me: {{Novices}} Represent Shared Goals in Joint Action},
  shorttitle = {The Sound of You and Me},
  author = {Loehr, Janeen D. and Vesper, Cordula},
  date = {2016-03},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  shortjournal = {Quarterly Journal of Experimental Psychology},
  volume = {69},
  number = {3},
  pages = {535--547},
  issn = {1747-0218, 1747-0226},
  doi = {10.1080/17470218.2015.1061029},
  url = {http://journals.sagepub.com/doi/10.1080/17470218.2015.1061029},
  urldate = {2022-12-18},
  abstract = {People performing joint actions coordinate their individual actions with each other to achieve a shared goal. The current study investigated the mental representations that are formed when people learn a new skill as part of a joint action. In a musical transfer-of-learning paradigm, piano novices first learned to perform simple melodies in the joint action context of coordinating with an accompanist to produce musical duets. Participants then performed their previously learned actions with two types of auditory feedback: while hearing either their individual action goal (the melody) or the shared action goal (the duet). As predicted, participants made more performance errors in the individual goal condition than in the shared goal condition. Further experimental manipulations indicated that this difference was not due to different coordination requirements in the two conditions or perceptual dissimilarities between learning and test. Together, these findings indicate that people form representations of shared goals in contexts that promote minimal representations, such as when learning a new action together with another person.},
  langid = {english},
  keywords = {extracted,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\UMASTSSV\\Loehr and Vesper - 2016 - The sound of you and me Novices represent shared .pdf}
}

@article{marshSocialConnectionJoint2009,
  title = {Social {{Connection Through Joint Action}} and {{Interpersonal Coordination}}},
  author = {Marsh, Kerry L. and Richardson, Michael J. and Schmidt, R. C.},
  date = {2009-04},
  journaltitle = {Topics in Cognitive Science},
  volume = {1},
  number = {2},
  pages = {320--339},
  issn = {17568757, 17568765},
  doi = {10.1111/j.1756-8765.2009.01022.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2009.01022.x},
  urldate = {2022-11-02},
  langid = {english},
  keywords = {highlighted,introduction,linus,methods},
  file = {C\:\\Users\\webma\\Zotero\\storage\\27IWFI3S\\Marsh et al. - 2009 - Social Connection Through Joint Action and Interpe.pdf}
}

@article{mcellinSynchronicitiesThatShape2020,
  title = {Synchronicities That Shape the Perception of Joint Action},
  author = {McEllin, Luke and Knoblich, GÃ¼nther and Sebanz, Natalie},
  date = {2020-09-23},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {15554},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-72729-6},
  url = {https://www.nature.com/articles/s41598-020-72729-6},
  urldate = {2022-10-09},
  abstract = {In joint performances spanning from jazz improvisation to soccer, expert performers synchronize their movements in ways that novices cannot. Particularly, experts can align the velocity profiles of their movements in order to achieve synchrony on a fine-grained time scale, compared to novices who can only synchronize the duration of their movement intervals. This study investigated how expertsâ ability to engage in velocity-based synchrony affects observersâ perception of coordination and their aesthetic experience of joint performances. Participants observed two moving dots on a screen and were told that these reflect the hand movements of two performers engaging in joint improvisation. The dots were animated to reflect the velocity-based synchrony characteristic of expert performance (in terms of jitter of the velocity profile: Experiment 1, or through aligning sharpness of the velocity profile: Experiment 2) or contained only interval-based synchrony. Performances containing velocity-based synchrony were judged as more coordinated with performers rated as liking each other more, and were rated as more beautiful, providing observers with a stronger aesthetic experience. These findings demonstrate that subtle timing cues fundamentally shape the experience of watching joint actions, directly influencing how beautiful and enjoyable we find these interactions, as well as our perception of the relationship between co-actors.},
  langid = {english},
  keywords = {linus-skim,Neuroscience,Psychology},
  file = {C\:\\Users\\webma\\Zotero\\storage\\335D5LVI\\McEllin et al. - 2020 - Synchronicities that shape the perception of joint action.pdf;C\:\\Users\\webma\\Zotero\\storage\\MY2TNPW4\\McEllin et al_2020_Synchronicities that shape the perception of joint action.pdf}
}

@article{mcphersonActionSoundLatencyAre2016,
  title = {Action-{{Sound Latency}}: {{Are Our Tools Fast Enough}}?},
  shorttitle = {Action-{{Sound Latency}}},
  author = {McPherson, A. P. and Jack, R. H. and Moro, G. and Proceedings of the International Conference on New Interfaces for Musical Expression, Brisbane},
  date = {2016-07-11},
  publisher = {{Griffith University}},
  url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/12479},
  urldate = {2022-10-31},
  abstract = {The importance of low and consistent latency in interactive music systems is well-established. So how do commonly-used tools for creating digital musical instruments and other tangible interfaces perform in terms of latency from user action to sound output? This paper examines several common configurations where a microcontroller (e.g. Arduino) or wireless device communicates with computer-based sound generator (e.g. Max/MSP, Pd). We find that, perhaps surprisingly, almost none of the tested configurations meet generally-accepted guidelines for latency and jitter. To address this limitation, the paper presents a new embedded platform, Bela, which is capable of complex audio and sensor processing at submillisecond latency.},
  langid = {english},
  annotation = {Accepted: 2016-05-24T10:51:01Z},
  file = {C\:\\Users\\webma\\Zotero\\storage\\IYIYJZKB\\McPherson et al_2016_Action-Sound Latency.pdf;C\:\\Users\\webma\\Zotero\\storage\\7FF6KL99\\12479.html}
}

@inproceedings{mcphersonEnvironmentSubmillisecondLatencyAudio2015,
  title = {An {{Environment}} for {{Submillisecond-Latency Audio}} and {{Sensor Processing}} on {{BeagleBone Black}}},
  author = {McPherson, Andrew and Zappi, Victor},
  date = {2015-05-06},
  publisher = {{Audio Engineering Society}},
  url = {https://www.aes.org/e-lib/browse.cfm?elib=17755},
  urldate = {2022-10-31},
  abstract = {This paper presents a new environment for ultra-low-latency processing of audio and sensor data on embedded hardware. The platform, which is targeted at digital musical instruments and audio effects, is based on the low-cost BeagleBone Black single-board computer. A custom expansion board features stereo audio and 8 channels each of 16-bit ADC and 16-bit DAC for sensors and actuators. In contrast to typical embedded Linux approaches, the platform uses the Xenomai real-time kernel extensions to...},
  eventtitle = {Audio {{Engineering Society Convention}} 138},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\I66ZAKFM\\browse.html}
}

@article{mullerHowOrchestrateSoccer2022,
  title = {How to Orchestrate a Soccer Team: {{Generalized}} Synchronization Promoted by Rhythmic Acoustic Stimuli},
  shorttitle = {How to Orchestrate a Soccer Team},
  author = {MÃ¼ller, Manfred A. and MartÃ­nez-Guerrero, Antonieta and Corsi-Cabrera, Maria and Effenberg, Alfred O. and Friedrich, Armin and Garcia-Madrid, Ignacio and Hornschuh, Matthias and Schmitz, Gerd and MÃ¼ller, Markus F.},
  date = {2022},
  journaltitle = {Frontiers in Human Neuroscience},
  volume = {16},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2022.909939},
  urldate = {2022-09-01},
  abstract = {Interpersonal coordination requires precise actions concerted in space and time in a self-organized manner. We found, using soccer teams as a testing ground, that a common timeframe provided by adequate acoustic stimuli improves the interplay between teammates. We provide quantitative evidence that the connectivity between teammates and the scoring rate of male soccer teams improve significantly when playing under the influence of an appropriate acoustic environment. Unexpectedly, female teams do not show any improvement under the same experimental conditions. We show by follow-up experiments that the acoustic rhythm modulates the attention level of the participants with a pronounced tempo preference and a marked gender difference in the preferred tempo. These results lead to a consistent explanation in terms of the dynamical system theory, nonlinear resonances, and dynamic attention theory, which may illuminate generic mechanisms of the brain dynamics and may have an impact on the design of novel training strategies in team sports.},
  file = {C\:\\Users\\webma\\Zotero\\storage\\J4A753Z9\\MÃ¼ller et al. - 2022 - How to orchestrate a soccer team Generalized sync.pdf}
}

@article{noyBeingZonePhysiological2015,
  title = {Being in the Zone: Physiological Markers of Togetherness in Joint Improvisation},
  shorttitle = {Being in the Zone},
  author = {Noy, Lior and Levit-Binun, Nava and Golland, Yulia},
  date = {2015-05-05},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {9},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2015.00187},
  url = {http://www.frontiersin.org/Human_Neuroscience/10.3389/fnhum.2015.00187/abstract},
  urldate = {2022-10-09},
  keywords = {linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\KGKJ9LWL\\Noy et al. - 2015 - Being in the zone_ physiological markers of togetherness in joint improvisation.pdf;C\:\\Users\\webma\\Zotero\\storage\\V5NI77E8\\Noy et al_2015_Being in the zone.pdf}
}

@article{noyMirrorGameParadigm2011,
  title = {The Mirror Game as a Paradigm for Studying the Dynamics of Two People Improvising Motion Together},
  author = {Noy, Lior and Dekel, Erez and Alon, Uri},
  date = {2011-12-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {108},
  number = {52},
  pages = {20947--20952},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1108155108},
  url = {https://pnas.org/doi/full/10.1073/pnas.1108155108},
  urldate = {2022-10-09},
  abstract = {Joint improvisation is the creative action of two or more people without a script or designated leader. Examples include improvisational theater and music, and day-to-day activities such as conversations. In joint improvisation, novel action is created, emerging from the interaction between people. Although central to creative processes and social interaction, joint improvisation remains largely unexplored due to the lack of experimental paradigms. Here we introduce a paradigm based on a theater practice called the mirror game. We measured the hand motions of two people mirroring each other at high temporal and spatial resolution. We focused on expert actors and musicians skilled in joint improvisation. We found that players can jointly create novel complex motion without a designated leader, synchronized to less than 40 ms. In contrast, we found that designating one player as leader deteriorated performance: The follower showed 2â3 Hz oscillation around the leader's smooth trajectory, decreasing synchrony and reducing the range of velocities reached. A mathematical model suggests a mechanism for these observations based on mutual agreement on future motion in mirrored reactiveâpredictive controllers. This is a step toward understanding the human ability to create novelty by improvising together.},
  langid = {english},
  keywords = {linus-skim},
  file = {C\:\\Users\\webma\\Zotero\\storage\\EY5IYM6N\\Noy_et_al_2011_The_mirror_game_as_a_paradigm_for_studying_the_dynamics_of_two_people.pdf}
}

@article{rizzolattiMIRRORNEURONSYSTEM2004,
  title = {{{THE MIRROR-NEURON SYSTEM}}},
  author = {Rizzolatti, Giacomo and Craighero, Laila},
  date = {2004-07-21},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {27},
  number = {1},
  pages = {169--192},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.27.070203.144230},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144230},
  urldate = {2022-11-04},
  abstract = {âª Abstract\hspace{0.6em} A category of stimuli of great importance for primates, humans in particular, is that formed by actions done by other individuals. If we want to survive, we must understand the actions of others. Furthermore, without action understanding, social organization is impossible. In the case of humans, there is another faculty that depends on the observation of others' actions: imitation learning. Unlike most species, we are able to learn by imitation, and this faculty is at the basis of human culture. In this review we present data on a neurophysiological mechanismâthe mirror-neuron mechanismâthat appears to play a fundamental role in both action understanding and imitation. We describe first the functional properties of mirror neurons in monkeys. We review next the characteristics of the mirror-neuron system in humans. We stress, in particular, those properties specific to the human mirror-neuron system that might explain the human capacity to learn by imitation. We conclude by discussing the relationship between the mirror-neuron system and language.},
  langid = {english},
  keywords = {linus,single-citation},
  file = {C\:\\Users\\webma\\Zotero\\storage\\P2TIJ59L\\Rizzolatti and Craighero - 2004 - THE MIRROR-NEURON SYSTEM.pdf}
}

@article{roguinReneTheophileHyacinthe2006,
  title = {Rene {{Theophile Hyacinthe Laennec}} (1781-1826): {{The Man Behind}} the {{Stethoscope}}},
  shorttitle = {Rene {{Theophile Hyacinthe Laennec}} (1781-1826)},
  author = {Roguin, A.},
  date = {2006-09-01},
  journaltitle = {Clinical Medicine \& Research},
  shortjournal = {Clinical Medicine \& Research},
  volume = {4},
  number = {3},
  pages = {230--235},
  issn = {1539-4182, 1554-6179},
  doi = {10.3121/cmr.4.3.230},
  url = {http://www.clinmedres.org/cgi/doi/10.3121/cmr.4.3.230},
  urldate = {2022-10-10},
  langid = {english},
  keywords = {linus,single-citation},
  file = {C\:\\Users\\webma\\Zotero\\storage\\KPVSRQ94\\Roguin - 2006 - Rene Theophile Hyacinthe Laennec (1781-1826) The .pdf}
}

@book{santosComparativeLatencyAnalysis2021,
  title = {Comparative {{Latency Analysis}} of {{Optical}} and {{Inertial Motion Capture Systems}} for {{Gestural Analysis}} and {{Musical Performance}}},
  author = {Santos, Geise and Wang, Johnty and Brum, Carolina and Wanderley, Marcelo and Fernandes Tavares, Tiago and Rocha, Anderson},
  date = {2021-06-01},
  doi = {10.21428/92fbeb44.51b1c3a1},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\CSZHQ46C\\Santos et al. - 2021 - Comparative Latency Analysis of Optical and Inerti.pdf}
}

@article{schmitzObservationSonifiedMovements2013,
  title = {Observation of Sonified Movements Engages a Basal Ganglia Frontocortical Network},
  author = {Schmitz, Gerd and Mohammadi, Bahram and Hammer, Anke and Heldmann, Marcus and Samii, Amir and MÃ¼nte, Thomas F. and Effenberg, Alfred O.},
  date = {2013-03-14},
  journaltitle = {BMC Neuroscience},
  shortjournal = {BMC Neurosci},
  volume = {14},
  number = {1},
  pages = {32},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-14-32},
  url = {https://doi.org/10.1186/1471-2202-14-32},
  urldate = {2022-09-01},
  abstract = {Producing sounds by a musical instrument can lead to audiomotor coupling, i.e. the joint activation of the auditory and motor system, even when only one modality is probed. The sonification of otherwise mute movements by sounds based on kinematic parameters of the movement has been shown to improve motor performance and perception of movements.},
  langid = {english},
  keywords = {Brodmann Area,Incongruent Stimulus,Mirror Neuron System,Multisensory Integration,Superior Temporal Sulcus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\J2XMWMQF\\Schmitz et al. - 2013 - Observation of sonified movements engages a basal .pdf}
}

@article{schmitzPERCEPTUALEFFECTSAUDITORY2012,
  title = {{{PERCEPTUAL EFFECTS OF AUDITORY INFORMATION ABOUT OWN AND OTHER MOVEMENTS}}},
  author = {Schmitz, Gerd and Effenberg, Alfred O},
  date = {2012},
  pages = {6},
  abstract = {In sport accurate predictions of other personsâ movements are essential. Former studies have shown that predictions can be enhanced by mapping movements onto sound (sonification) and providing audiovisual feedback [1]. The present study investigated behavioral mechanisms of movement sonification and scrutinized whether effects of own movements and those of other persons can be predicted just by listening to them. Eight athletes heard sonifications of an indoor rower and quantified resulting velocities of a virtual boat. Although boat velocity was not mapped onto sound directly, it explained subjectsâ quantifications by regression analysis (R-squared = 0.80) significantly better than the directly sonified amplitude and force parameters. Thus perception of boat velocity might have emerged from those sonifications. Predictions of effects of unknown movements were above chance level and as good as predictions of own movements. Furthermore athletes were able to identify their own technique among others ( dâ = 0.47 Â± 0.43). The results confirm large perceptual effects of auditory feedback and - most importantly - suggest that movement sonification can address central motor representations just by listening to it. Therefore not only predictability but also synchronization with other personsâ movements might be supported.},
  langid = {english},
  keywords = {linus,luke,useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\N37CUJK7\\Schmitz and Effenberg - 2012 - PERCEPTUAL EFFECTS OF AUDITORY INFORMATION ABOUT O.pdf}
}

@article{sebanzJointActionBodies2006,
  title = {Joint Action: Bodies and Minds Moving Together},
  shorttitle = {Joint Action},
  author = {Sebanz, N and Bekkering, H and Knoblich, G},
  date = {2006-02},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {2},
  pages = {70--76},
  issn = {13646613},
  doi = {10.1016/j.tics.2005.12.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661305003566},
  urldate = {2022-10-27},
  langid = {english},
  keywords = {highlighted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\2IH5VK5A\\Sebanz et al. - 2006 - Joint action bodies and minds moving together.pdf}
}

@article{tomaselloCultureCognitiveDevelopment2000,
  title = {Culture and {{Cognitive Development}}},
  author = {Tomasello, Michael},
  date = {2000-04},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {9},
  number = {2},
  pages = {37--40},
  issn = {0963-7214, 1467-8721},
  doi = {10.1111/1467-8721.00056},
  url = {http://journals.sagepub.com/doi/10.1111/1467-8721.00056},
  urldate = {2022-10-27},
  abstract = {Human beings are biologically adapted for culture in ways that other primates are not. The difference can be clearly seen when the social learning skills of humans and their nearest primate relatives are systematically compared. The human adaptation for culture begins to make itself manifest in human ontogeny at around 1 year of age as human infants come to understand other persons as intentional agents like the self and so engage in joint attentional interactions with them. This understanding then enables young children (a) to employ some uniquely powerful forms of cultural learning to acquire the accumulated wisdom of their cultures, especially as embodied in language, and also (b) to comprehend their worlds in some uniquely powerful ways involving perspectivally based symbolic representations.},
  langid = {english},
  keywords = {single-citation},
  file = {C\:\\Users\\webma\\Zotero\\storage\\P3PAFP8J\\Tomasello - 2000 - Culture and Cognitive Development.pdf}
}

@article{umekSuitabilityStrainGage2017,
  title = {Suitability of {{Strain Gage Sensors}} for {{Integration}} into {{Smart Sport Equipment}}: {{A Golf Club Example}}},
  shorttitle = {Suitability of {{Strain Gage Sensors}} for {{Integration}} into {{Smart Sport Equipment}}},
  author = {Umek, Anton and Zhang, Yuan and TomaÅ¾iÄ, SaÅ¡o and Kos, Anton},
  date = {2017-04-21},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {17},
  number = {4},
  pages = {916},
  issn = {1424-8220},
  doi = {10.3390/s17040916},
  url = {http://www.mdpi.com/1424-8220/17/4/916},
  urldate = {2022-09-21},
  abstract = {Wearable devices and smart sport equipment are being increasingly used in amateur and professional sports. Smart sport equipment employs various sensors for detecting its state and actions. The correct choice of the most appropriate sensor(s) is of paramount importance for efficient and successful operation of sport equipment. When integrated into the sport equipment, ideal sensors are unobstructive, and do not change the functionality of the equipment. The article focuses on experiments for identification and selection of sensors that are suitable for the integration into a golf club with the final goal of their use in real time biofeedback applications. We tested two orthogonally affixed strain gage (SG) sensors, a 3-axis accelerometer, and a 3-axis gyroscope. The strain gage sensors are calibrated and validated in the laboratory environment by a highly accurate Qualisys Track Manager (QTM) optical tracking system. Field test results show that different types of golf swing and improper movement in early phases of golf swing can be detected with strain gage sensors attached to the shaft of the golf club. Thus they are suitable for biofeedback applications to help golfers to learn repetitive golf swings. It is suggested that the use of strain gage sensors can improve the golf swing technical error detection accuracy and that strain gage sensors alone are enough for basic golf swing analysis. Our final goal is to be able to acquire and analyze as many parameters of a smart golf club in real time during the entire duration of the swing. This would give us the ability to design mobile and cloud biofeedback applications with terminal or concurrent feedback that will enable us to speed-up motor skill learning in golf.},
  langid = {english},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\ENHX8XEA\\Umek et al. - 2017 - Suitability of Strain Gage Sensors for Integration.pdf}
}

@article{valdesoloRhythmJointAction2010,
  title = {The Rhythm of Joint Action: {{Synchrony}} Promotes Cooperative Ability},
  shorttitle = {The Rhythm of Joint Action},
  author = {Valdesolo, Piercarlo and Ouyang, Jennifer and DeSteno, David},
  date = {2010-07},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  volume = {46},
  number = {4},
  pages = {693--695},
  issn = {00221031},
  doi = {10.1016/j.jesp.2010.03.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103110000430},
  urldate = {2022-11-02},
  langid = {english},
  keywords = {highlighted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\593KPNBC\\Valdesolo et al. - 2010 - The rhythm of joint action Synchrony promotes coo.pdf}
}

@article{vanderwelUnderstandingJointAction2021,
  title = {Understanding Joint Action: {{Current}} Theoretical and Empirical Approaches},
  shorttitle = {Understanding Joint Action},
  author = {van der Wel, Robrecht P.R.D. and Becchio, Cristina and Curioni, Arianna and Wolf, Thomas},
  options = {useprefix=true},
  date = {2021-04},
  journaltitle = {Acta Psychologica},
  shortjournal = {Acta Psychologica},
  volume = {215},
  pages = {103285},
  issn = {00016918},
  doi = {10.1016/j.actpsy.2021.103285},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0001691821000354},
  urldate = {2022-10-27},
  langid = {english},
  keywords = {highlighted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\YEBMR3W2\\van der Wel et al. - 2021 - Understanding joint action Current theoretical an.pdf}
}

@article{varletComputationContinuousRelative2011a,
  title = {Computation of Continuous Relative Phase and Modulation of Frequency of Human Movement},
  author = {Varlet, Manuel and Richardson, Michael J.},
  date = {2011-04-07},
  journaltitle = {Journal of Biomechanics},
  shortjournal = {Journal of Biomechanics},
  volume = {44},
  number = {6},
  pages = {1200--1204},
  issn = {0021-9290},
  doi = {10.1016/j.jbiomech.2011.02.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0021929011000820},
  urldate = {2022-12-19},
  abstract = {Continuous relative phase measures have been used to quantify the coordination between different body segments in several activities. Our aim in this study was to investigate how the methods traditionally used to compute the continuous phase of human rhythmic movement are affected by modulations of frequency. We compared the continuous phase computed method with the traditional method derived from the positionâvelocity phase plane and with the Hilbert Transform. The methods were tested using sinusoidal signals with a modulation of frequency between or within cycles. Our results showed that the continuous phase computed with the first method results in oscillations in the phase time-series not expected for a sinusoidal signal and that the continuous phase is overestimated with the Hilbert Transform. We proposed a new method that produces a correct estimation of continuous phase by using half-cycle estimations of frequency to normalize the phase planes prior to calculating phase angles. The findings of the current study have important implications for computing continuous relative phase when investigating human movement coordination.},
  langid = {english},
  keywords = {Continuous relative phase,Coordination,Frequency modulation,Hilbert Transform,Normalization,Velocity},
  file = {C\:\\Users\\webma\\Zotero\\storage\\ZHZJF87R\\Varlet_Richardson_2011_Computation of continuous relative phase and modulation of frequency of human.pdf;C\:\\Users\\webma\\Zotero\\storage\\5FIPCLKZ\\S0021929011000820.html}
}

@article{vesperMinimalArchitectureJoint2010,
  title = {A Minimal Architecture for Joint Action},
  author = {Vesper, Cordula and Butterfill, Stephen and Knoblich, GÃ¼nther and Sebanz, Natalie},
  date = {2010-10},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {23},
  number = {8-9},
  pages = {998--1003},
  issn = {08936080},
  doi = {10.1016/j.neunet.2010.06.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608010001152},
  urldate = {2022-12-14},
  abstract = {What kinds of processes and representations make joint action possible? In this paper, we suggest a minimal architecture for joint action that focuses on representations, action monitoring and action prediction processes, as well as ways of simplifying coordination. The architecture spells out minimal requirements for an individual agent to engage in a joint action. We discuss existing evidence in support of the architecture as well as open questions that remain to be empirically addressed. In addition, we suggest possible interfaces between the minimal architecture and other approaches to joint action. The minimal architecture has implications for theorising about the emergence of joint action, for humanâmachine interaction, and for understanding how coordination can be facilitated by exploiting relations between multiple agentsâ actions and between actions and the environment.},
  langid = {english},
  keywords = {extracted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\E3BUU2Q6\\Vesper et al. - 2010 - A minimal architecture for joint action.pdf}
}

@article{vinkenAuditoryCodingHuman2013,
  title = {Auditory {{Coding}} of {{Human Movement Kinematics}}},
  author = {Vinken, Pia M. and KrÃ¶ger, Daniela and Fehse, Ursula and Schmitz, Gerd and Brock, Heike and Effenberg, Alfred~O.},
  date = {2013},
  journaltitle = {Multisensory Research},
  shortjournal = {Multisens Res},
  volume = {26},
  number = {6},
  pages = {533--552},
  issn = {2213-4794, 2213-4808},
  doi = {10.1163/22134808-00002435},
  url = {https://brill.com/view/journals/msr/26/6/article-p533_2.xml},
  urldate = {2022-09-01},
  abstract = {Although visual perception is dominant on motor perception, control and learning, auditory information can enhance and modulate perceptual as well as motor processes in a multifaceted manner. During last decades new methods of auditory augmentation had been developed with movement sonification as one of the most recent approaches expanding auditory movement information also to usually mute phases of movement. Despite general evidence on the effectiveness of movement sonification in different fields of applied research there is nearly no empirical proof on how sonification of gross motor human movement should be configured to achieve information rich sound sequences. Such lack of empirical proof is given for (a) the selection of suitable movement features as well as for (b) effective kineticâacoustical mapping patterns and for (c) the number of regarded dimensions of sonification. In this study we explore the informational content of artificial acoustical kinematics in terms of a kinematic movement sonification using an intermodal discrimination paradigm. In a repeated measure design we analysed discrimination rates of six everyday upper limb actions to evaluate the effectiveness of seven different kinds of kinematicâacoustical mappings as well as short term learning effects. The kinematics of the upper limb actions were calculated based on inertial motion sensor data and transformed into seven different sonifications. Sound sequences were randomly presented to participants and discrimination rates as well as confidence of choice were analysed. Data indicate an instantaneous comprehensibility of the artificial movement acoustics as well as short term learning effects. No differences between different dimensional encodings became evident thus indicating a high efficiency for intermodal pattern discrimination for the acoustically coded velocity distribution of the actions. Taken together movement information related to continuous kinematic parameters can be transformed into the auditory domain. Additionally, pattern based action discrimination is obviously not restricted to the visual modality. Artificial acoustical kinematics might be used to supplement and/or substitute visual motion perception in sports and motor rehabilitation.},
  langid = {english},
  keywords = {linus,luke,useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\DII8GFIJ\\Vinken et al. - 2013 - Auditory Coding of Human Movement Kinematics.pdf}
}

@article{walkerMUSICALSOUNDSCAPESACCESSIBLE2007,
  title = {{{MUSICAL SOUNDSCAPES FOR AN ACCESSIBLE AQUARIUM}}: {{BRINGING DYNAMIC EXHIBITS TO THE VISUALLY IMPAIRED}}},
  author = {Walker, Bruce N and Kim, Jonathan and Pendse, Anandi},
  date = {2007},
  journaltitle = {Proceedings of the 2007 International Computer Music Conference, ICMC 2007, Copenhagen, Denmark, August 27-31, 2007.},
  pages = {8},
  abstract = {In an effort to make an aquarium, zoo, or other dynamic âinformal learning environmentâ more accessible to the visually impaired, we track the fish (and other creatures) with computer vision, then use the movement data to create meaningful and aesthetic music. Here we present four new classes of âsoundscapesâ, which demonstrate a range of data-to-music mapping approaches. This follows on the initial prototype work, discussed previously. A systematic exploration of the possible composition and design space is leading to music that communicates the dynamic aspects of the exhibit (e.g., how many fish, what kinds, where they are, what they are doing), as well as conveying the emotional content (e.g., amazement and wonder at the massive whale shark gliding by). Informal evaluations have been very successful; formal evaluations are ongoing.},
  langid = {english},
  keywords = {linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\GE5MJGB4\\Walker et al. - MUSICAL SOUNDSCAPES FOR AN ACCESSIBLE AQUARIUM BR.pdf}
}

@article{wiltermuthSynchronyCooperation2009,
  title = {Synchrony and {{Cooperation}}},
  author = {Wiltermuth, Scott S. and Heath, Chip},
  date = {2009-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {20},
  number = {1},
  pages = {1--5},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2008.02253.x},
  url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.2008.02253.x},
  urldate = {2022-11-02},
  abstract = {Armies, churches, organizations, and communities often engage in activitiesâfor example, marching, singing, and dancingâthat lead group members to act in synchrony with each other. Anthropologists and sociologists have speculated that rituals involving synchronous activity may produce positive emotions that weaken the psychological boundaries between the self and the group. This article explores whether synchronous activity may serve as a partial solution to the free-rider problem facing groups that need to motivate their members to contribute toward the collective good. Across three experiments, people acting in synchrony with others cooperated more in subsequent group economic exercises, even in situations requiring personal sacrifice. Our results also showed that positive emotions need not be generated for synchrony to foster cooperation. In total, the results suggest that acting in synchrony with others can increase cooperation by strengthening social attachment among group members.},
  langid = {english},
  keywords = {highlighted,introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\F4YPMGYL\\Wiltermuth and Heath - 2009 - Synchrony and Cooperation.pdf}
}

@article{zammSynchronizingMIDIWireless2019,
  title = {Synchronizing {{MIDI}} and Wireless {{EEG}} Measurements during Natural Piano Performance},
  author = {Zamm, Anna and Palmer, Caroline and Bauer, Anna-Katharina R. and Bleichner, Martin G. and Demos, Alexander P. and Debener, Stefan},
  date = {2019-08},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  volume = {1716},
  pages = {27--38},
  issn = {00068993},
  doi = {10.1016/j.brainres.2017.07.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899317302913},
  urldate = {2022-09-14},
  langid = {english},
  keywords = {methods,useful,useful-structure},
  file = {C\:\\Users\\webma\\Zotero\\storage\\DD2MAP6N\\Zamm et al. - 2019 - Synchronizing MIDI and wireless EEG measurements d.pdf}
}
