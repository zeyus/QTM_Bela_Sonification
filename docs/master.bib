
@book{almeidaDevelopmentWearableSensor2012,
  title = {Development of a Wearable Sensor System for Real-Time Control of Knee Prostheses},
  author = {de Almeida, Eduardo Carlos Venancio},
  date = {2012},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-81244},
  urldate = {2022-09-21},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\WCDV26XV\\Almeida - 2012 - Development of a wearable sensor system for real-t.pdf;C\:\\Users\\webma\\Zotero\\storage\\KIPBEW4J\\record.html}
}

@article{brockIfMotionSounds2012,
  title = {If Motion Sounds: {{Movement}} Sonification Based on Inertial Sensor Data},
  shorttitle = {If Motion Sounds},
  author = {Brock, Heike and Schmitz, Gerd and Baumann, Jan and Effenberg, Alfred O.},
  date = {2012},
  journaltitle = {Procedia Engineering},
  shortjournal = {Procedia Engineering},
  volume = {34},
  pages = {556--561},
  issn = {18777058},
  doi = {10.1016/j.proeng.2012.04.095},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877705812017080},
  urldate = {2022-09-01},
  abstract = {Within last years, movement sonification turned out to be an appropriate support for motor perception and motor control that can display physical motion in a very rich and direct way. But how should movement sonification be configured to support motor learning? The appropriate selection of movement parameters and their transformation into characteristic motion features is essential for an auditory display to become effective. In this paper, we introduce a real-time sonification framework for all common MIDI environments based on acceleration and orientation data from inertial sensors. Fundamental processing steps to transform motion information into meaningful sound will be discussed. The proposed framework of inertial motion capturing, kinematic parameter selection and possible kinematic acoustic mapping provides a basis for mobile real-time movement sonification which is a prospective powerful training tool for rehabilitation and sports and offers a broad variety of application possibilities.},
  langid = {english},
  keywords = {linus,luke},
  file = {C\:\\Users\\webma\\Zotero\\storage\\HYCVDNZ8\\Brock et al. - 2012 - If motion sounds Movement sonification based on i.pdf}
}

@article{demosRockingBeatEffects2012,
  title = {Rocking to the Beat: {{Effects}} of Music and Partner's Movements on Spontaneous Interpersonal Coordination.},
  shorttitle = {Rocking to the Beat},
  author = {Demos, Alexander P. and Chaffin, Roger and Begosh, Kristen T. and Daniels, Jennifer R. and Marsh, Kerry L.},
  date = {2012-02},
  journaltitle = {Journal of Experimental Psychology: General},
  shortjournal = {Journal of Experimental Psychology: General},
  volume = {141},
  number = {1},
  pages = {49--53},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0023843},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0023843},
  urldate = {2022-10-15},
  langid = {english},
  keywords = {introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\HN8UYBSS\\Demos et al. - 2012 - Rocking to the beat Effects of music and partner'.pdf}
}

@incollection{dixLatencyCyberPhysicalSystems2022,
  title = {Latency in {{Cyber-Physical Systems}}: {{The Role}} of {{Visual Feedback Delays}} on {{Manual Skill Learning}}},
  shorttitle = {Latency in {{Cyber-Physical Systems}}},
  author = {Dix, Annika and Helmert, Jens and Pannasch, Sebastian},
  date = {2022-01-01},
  pages = {1138--1146},
  doi = {10.1007/978-3-030-85540-6_146},
  abstract = {To inform, guide and optimize the design of cyber-physical systems (CPS), this study examined whether and how delayed visual feedback affects human fine-motor skills. Two experiments are presented, in which participants performed a complex motor task with their hands. During the task, visual feedback was provided on a display with varying delay lengths. Further, to investigate effects of adaptation and transfer, some participants were first exposed to a fixed delay before performing the task with varying delay lengths. Results show that independent of earlier delay exposure feedback delays had detrimental effects on performance, particularly when performance information was lacking and delay length variable. Hand kinematic indicate the use of a strategy geared to a slowing of the movement process. Implications and future research ideas like the application of augmented feedback on movement kinematic and the examination of different settings to promote adaptation effects are discussed.},
  isbn = {978-3-030-85539-0},
  keywords = {latency}
}

@article{dubusInteractiveSonificationMotion2013,
  title = {Interactive Sonification of Motion : {{Design}}, Implementation and Control of Expressive Auditory Feedback with Mobile Devices},
  shorttitle = {Interactive Sonification of Motion},
  author = {Dubus, Gaël},
  date = {2013},
  publisher = {{KTH Royal Institute of Technology}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-127944},
  urldate = {2022-09-05},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\V4JITEWG\\Dubus - 2013 - Interactive sonification of motion  Design, imple.pdf;C\:\\Users\\webma\\Zotero\\storage\\94UMW8VZ\\record.html}
}

@article{dubusSonificationPhysicalQuantities2011,
  title = {Sonification of {{Physical Quantities Throughout History}}: {{A Meta-Study}} of {{Previous Mapping Strategies}}},
  author = {Dubus, Gael and Bresin, Roberto},
  date = {2011-06},
  journaltitle = {International Conference on Auditory Display, 2011},
  pages = {8},
  abstract = {We introduce a meta-study of previous sonification designs taking physical quantities as input data. The aim is to build a solid foundation for future sonification works so that auditory display researchers would be able to take benefit from former studies, avoiding to start from scratch when beginning new sonification projects. This work is at an early stage and the objective of this paper is rather to introduce the methodology than to come to definitive conclusions. After a historical introduction, we explain how to collect a large amount of articles and extract useful information about mapping strategies. Then, we present the physical quantities grouped according to conceptual dimensions, as well as the sound parameters used in sonification designs and we summarize the current state of the study by listing the couplings extracted from the article database. A total of 54 articles have been examined for the present article. Finally, a preliminary analysis of the results is performed.},
  langid = {english},
  keywords = {introduction,linus,luke,useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\R5PXJFRK\\Dubus and Bresin - 2011 - SONIFICATION OF PHYSICAL QUANTITIES THROUGHOUT HIS.pdf}
}

@article{dubusSystematicReviewMapping2013,
  title = {A {{Systematic Review}} of {{Mapping Strategies}} for the {{Sonification}} of {{Physical Quantities}}},
  author = {Dubus, Gaël and Bresin, Roberto},
  date = {2013-12-17},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  number = {12},
  pages = {e82491},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0082491},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0082491},
  urldate = {2022-09-05},
  abstract = {The field of sonification has progressed greatly over the past twenty years and currently constitutes an established area of research. This article aims at exploiting and organizing the knowledge accumulated in previous experimental studies to build a foundation for future sonification works. A systematic review of these studies may reveal trends in sonification design, and therefore support the development of design guidelines. To this end, we have reviewed and analyzed 179 scientific publications related to sonification of physical quantities. Using a bottom-up approach, we set up a list of conceptual dimensions belonging to both physical and auditory domains. Mappings used in the reviewed works were identified, forming a database of 495 entries. Frequency of use was analyzed among these conceptual dimensions as well as higher-level categories. Results confirm two hypotheses formulated in a preliminary study: pitch is by far the most used auditory dimension in sonification applications, and spatial auditory dimensions are almost exclusively used to sonify kinematic quantities. To detect successful as well as unsuccessful sonification strategies, assessment of mapping efficiency conducted in the reviewed works was considered. Results show that a proper evaluation of sonification mappings is performed only in a marginal proportion of publications. Additional aspects of the publication database were investigated: historical distribution of sonification works is presented, projects are classified according to their primary function, and the sonic material used in the auditory display is discussed. Finally, a mapping-based approach for characterizing sonification is proposed.},
  langid = {english},
  keywords = {Drug synthesis,Kinematics,Physical mapping,Pitch perception,Sensory perception,Signal filtering,Sonification,Systematic reviews},
  file = {C\:\\Users\\webma\\Zotero\\storage\\ILLLBKVQ\\Dubus and Bresin - 2013 - A Systematic Review of Mapping Strategies for the .pdf;C\:\\Users\\webma\\Zotero\\storage\\3DTNDWBP\\article.html}
}

@article{effenbergAccelerationDecelerationConstant2018,
  title = {Acceleration and Deceleration at Constant Speed: Systematic Modulation of Motion Perception by Kinematic Sonification},
  shorttitle = {Acceleration and Deceleration at Constant Speed},
  author = {Effenberg, Alfred O. and Schmitz, Gerd},
  date = {2018-08},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann. N.Y. Acad. Sci.},
  volume = {1425},
  number = {1},
  pages = {52--69},
  issn = {0077-8923, 1749-6632},
  doi = {10.1111/nyas.13693},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/nyas.13693},
  urldate = {2022-09-01},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\QCZDNVFK\\Effenberg and Schmitz - 2018 - Acceleration and deceleration at constant speed s.pdf}
}

@incollection{gerdschmitzSoundJoinedActions2017,
  title = {Sound {{Joined Actions}} in {{Rowing}} and {{Swimming}}},
  booktitle = {Moving Bodies in Interaction-- Interacting Bodies in Motion: Intercorporeality, Interkinaesthesia, and Enaction in Sports},
  author = {{Gerd Schmitz} and {Alfred O. Effenberg}},
  editor = {Meyer, Christian and Wedelstaedt, Ulrich V.},
  date = {2017},
  series = {Advances in Interaction Studies},
  number = {volume 8},
  pages = {193--214},
  publisher = {{John Benjamins Publishing Company}},
  location = {{Amsterdam ; Philadelphia}},
  abstract = {The present chapter introduces the method of sonification as a tool for studying intercorporeality and enactment. We show that auditory movement information can support motor perception as well as the control of movements, and explain these effects by mechanisms which are consistent with the enactment approach. Providing additional auditory information about a movement enables the acting individual as well as observers to perceive the movement in exactly the same way via audition. Thus, a sonification can establish a common percept for all interaction partners, which corresponds well to the concept of intercorporeality. Furthermore, we show that sonifications can be specifically designed to constitute a variety of frameworks for the analysis of interpersonal coordination and intercorporeality.},
  isbn = {978-90-272-6555-5},
  keywords = {Athletes,Discourse analysis,Interaction (Philosophy),introduction,Kinesiology,linus,Movement; Psychology of,Physiological aspects,Physiology,Sports},
  file = {C\:\\Users\\webma\\Zotero\\storage\\SJNFT4JV\\Schmitz_Effenberg_Sound_Joined_Actions_final.pdf}
}

@article{hwangEffectPerformanceBasedAuditory2018,
  title = {Effect- and {{Performance-Based Auditory Feedback}} on {{Interpersonal Coordination}}},
  author = {Hwang, Tong-Hun and Schmitz, Gerd and Klemmt, Kevin and Brinkop, Lukas and Ghai, Shashank and Stoica, Mircea and Maye, Alexander and Blume, Holger and Effenberg, Alfred O.},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00404},
  urldate = {2022-09-01},
  abstract = {When two individuals interact in a collaborative task, such as carrying a sofa or a table, usually spatiotemporal coordination of individual motor behavior will emerge. In many cases, interpersonal coordination can arise independently of verbal communication, based on the observation of the partners' movements and/or the object's movements. In this study, we investigate how social coupling between two individuals can emerge in a collaborative task under different modes of perceptual information. A visual reference condition was compared with three different conditions with new types of additional auditory feedback provided in real time: effect-based auditory feedback, performance-based auditory feedback, and combined effect/performance-based auditory feedback. We have developed a new paradigm in which the actions of both participants continuously result in a seamlessly merged effect on an object simulated by a tablet computer application. Here, participants should temporally synchronize their movements with a 90° phase difference and precisely adjust the finger dynamics in order to keep the object (a ball) accurately rotating on a given circular trajectory on the tablet. Results demonstrate that interpersonal coordination in a joint task can be altered by different kinds of additional auditory information in various ways.},
  keywords = {useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\IQ3HXJFR\\Hwang et al. - 2018 - Effect- and Performance-Based Auditory Feedback on.pdf}
}

@inproceedings{kosBiofeedbackSportChallenges2015,
  title = {Biofeedback in Sport: {{Challenges}} in Real-Time Motion Tracking and Processing},
  shorttitle = {Biofeedback in Sport},
  booktitle = {2015 {{IEEE}} 15th {{International Conference}} on {{Bioinformatics}} and {{Bioengineering}} ({{BIBE}})},
  author = {Kos, Anton and Umek, Anton and Tomazic, Saso},
  date = {2015-11},
  pages = {1--4},
  doi = {10.1109/BIBE.2015.7367681},
  abstract = {Science and technology are ever more frequently used in sports for achieving the competitive advantage. Motion tracking systems, in connection to the biomechanical biofeedback, help in accelerating motor learning. Requirements about various parameters important in real-time biofeedback applications are discussed. Special focus is given on feedback loop delays and its real-time operation. Optical tracking and inertial sensor tracking systems are presented and compared. Real-time sensor signal acquisitions and real-time processing challenges, in connection to biomechanical biofeedback, are presented. This paper can serve as a starting point for determining the adequate combination of technical equipment and its specifications that work favorably for the operation of the planned real-time biofeedback application.},
  eventtitle = {2015 {{IEEE}} 15th {{International Conference}} on {{Bioinformatics}} and {{Bioengineering}} ({{BIBE}})},
  keywords = {Biological control systems,Biomedical optical imaging,Delays,Feedback loop,Gyroscopes,introduction,latency,Real-time systems,Tracking},
  file = {C\:\\Users\\webma\\Zotero\\storage\\PYVTBGBM\\Kos et al. - 2015 - Biofeedback in sport Challenges in real-time moti.pdf;C\:\\Users\\webma\\Zotero\\storage\\TVQCQ34F\\7367681.html}
}

@misc{kramerSonificationReportStatus1999,
  title = {Sonification {{Report}}: {{Status}} of the {{Field}} and {{Research Agenda}}},
  author = {Kramer, Gregory and Walker, Bruce and Bonebright, Terri and Cook, Perry},
  date = {1999},
  publisher = {{International Community for Auditory Display}},
  keywords = {introduction,linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\RDTBT4TR\\Kramer et al. - 1999 - Sonification Report Status of the Field and Resea.pdf}
}

@misc{krascekWebBasedElearning,
  title = {Web {{Based E-learning Tool}} for {{Visualization}} and {{Analysis}} of {{3D Motion Capture Data}}},
  author = {Krašček, Andraž and Stojmenova, Kristina and Tomažič, Sašo and Sodnik, Jaka},
  abstract = {Abstract—In this paper, we propose an e-learning tool for visualization and manipulation of 3D data on a web platform. The data is streamed in real time from an optical motion capture system Qualisys consisting of eight infrared cameras and Qualisys Track Manager (QTM) software. A WebSocket protocol and WebGL application programming interface (API) are used to visualize and to interact with the data in a browser. The tool represents a web-based extension of QTM software providing also additional features and new possibilities to manipulate and analyze the data. We report also on a user study in which we evaluated the web based application and compared it with the original desktop-based application. The proposed application proved to be fast, effective and intuitive and can be used as an e-learning tool for demonstrating and teaching techniques for visualization and analysis of motion capture data. Keywords-motion capture; Qualisys; e-learning; 3D data; AIM model; WebGL; WebSocket. I.},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\4JMEH4XF\\Krašček et al. - Web Based E-learning Tool for Visualization and An.pdf;C\:\\Users\\webma\\Zotero\\storage\\5QD5TBKB\\download.html}
}

@article{mcellinSynchronicitiesThatShape2020,
  title = {Synchronicities That Shape the Perception of Joint Action},
  author = {McEllin, Luke and Knoblich, Günther and Sebanz, Natalie},
  date = {2020-09-23},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {15554},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-72729-6},
  url = {https://www.nature.com/articles/s41598-020-72729-6},
  urldate = {2022-10-09},
  abstract = {In joint performances spanning from jazz improvisation to soccer, expert performers synchronize their movements in ways that novices cannot. Particularly, experts can align the velocity profiles of their movements in order to achieve synchrony on a fine-grained time scale, compared to novices who can only synchronize the duration of their movement intervals. This study investigated how experts’ ability to engage in velocity-based synchrony affects observers’ perception of coordination and their aesthetic experience of joint performances. Participants observed two moving dots on a screen and were told that these reflect the hand movements of two performers engaging in joint improvisation. The dots were animated to reflect the velocity-based synchrony characteristic of expert performance (in terms of jitter of the velocity profile: Experiment 1, or through aligning sharpness of the velocity profile: Experiment 2) or contained only interval-based synchrony. Performances containing velocity-based synchrony were judged as more coordinated with performers rated as liking each other more, and were rated as more beautiful, providing observers with a stronger aesthetic experience. These findings demonstrate that subtle timing cues fundamentally shape the experience of watching joint actions, directly influencing how beautiful and enjoyable we find these interactions, as well as our perception of the relationship between co-actors.},
  langid = {english},
  keywords = {Neuroscience,Psychology},
  file = {C\:\\Users\\webma\\Zotero\\storage\\MY2TNPW4\\McEllin et al_2020_Synchronicities that shape the perception of joint action.pdf}
}

@article{mullerHowOrchestrateSoccer2022,
  title = {How to Orchestrate a Soccer Team: {{Generalized}} Synchronization Promoted by Rhythmic Acoustic Stimuli},
  shorttitle = {How to Orchestrate a Soccer Team},
  author = {Müller, Manfred A. and Martínez-Guerrero, Antonieta and Corsi-Cabrera, Maria and Effenberg, Alfred O. and Friedrich, Armin and Garcia-Madrid, Ignacio and Hornschuh, Matthias and Schmitz, Gerd and Müller, Markus F.},
  date = {2022},
  journaltitle = {Frontiers in Human Neuroscience},
  volume = {16},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2022.909939},
  urldate = {2022-09-01},
  abstract = {Interpersonal coordination requires precise actions concerted in space and time in a self-organized manner. We found, using soccer teams as a testing ground, that a common timeframe provided by adequate acoustic stimuli improves the interplay between teammates. We provide quantitative evidence that the connectivity between teammates and the scoring rate of male soccer teams improve significantly when playing under the influence of an appropriate acoustic environment. Unexpectedly, female teams do not show any improvement under the same experimental conditions. We show by follow-up experiments that the acoustic rhythm modulates the attention level of the participants with a pronounced tempo preference and a marked gender difference in the preferred tempo. These results lead to a consistent explanation in terms of the dynamical system theory, nonlinear resonances, and dynamic attention theory, which may illuminate generic mechanisms of the brain dynamics and may have an impact on the design of novel training strategies in team sports.},
  file = {C\:\\Users\\webma\\Zotero\\storage\\J4A753Z9\\Müller et al. - 2022 - How to orchestrate a soccer team Generalized sync.pdf}
}

@article{noyBeingZonePhysiological2015,
  title = {Being in the Zone: Physiological Markers of Togetherness in Joint Improvisation},
  shorttitle = {Being in the Zone},
  author = {Noy, Lior and Levit-Binun, Nava and Golland, Yulia},
  date = {2015-05-05},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {9},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2015.00187},
  url = {http://www.frontiersin.org/Human_Neuroscience/10.3389/fnhum.2015.00187/abstract},
  urldate = {2022-10-09},
  file = {C\:\\Users\\webma\\Zotero\\storage\\V5NI77E8\\Noy et al_2015_Being in the zone.pdf}
}

@article{noyMirrorGameParadigm2011,
  title = {The Mirror Game as a Paradigm for Studying the Dynamics of Two People Improvising Motion Together},
  author = {Noy, Lior and Dekel, Erez and Alon, Uri},
  date = {2011-12-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {108},
  number = {52},
  pages = {20947--20952},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1108155108},
  url = {https://pnas.org/doi/full/10.1073/pnas.1108155108},
  urldate = {2022-10-09},
  abstract = {Joint improvisation is the creative action of two or more people without a script or designated leader. Examples include improvisational theater and music, and day-to-day activities such as conversations. In joint improvisation, novel action is created, emerging from the interaction between people. Although central to creative processes and social interaction, joint improvisation remains largely unexplored due to the lack of experimental paradigms. Here we introduce a paradigm based on a theater practice called the mirror game. We measured the hand motions of two people mirroring each other at high temporal and spatial resolution. We focused on expert actors and musicians skilled in joint improvisation. We found that players can jointly create novel complex motion without a designated leader, synchronized to less than 40 ms. In contrast, we found that designating one player as leader deteriorated performance: The follower showed 2–3 Hz oscillation around the leader's smooth trajectory, decreasing synchrony and reducing the range of velocities reached. A mathematical model suggests a mechanism for these observations based on mutual agreement on future motion in mirrored reactive–predictive controllers. This is a step toward understanding the human ability to create novelty by improvising together.},
  langid = {english},
  file = {C\:\\Users\\webma\\Zotero\\storage\\CVR7JYIL\\Noy et al_2011_The mirror game as a paradigm for studying the dynamics of two people.pdf}
}

@article{roguinReneTheophileHyacinthe2006,
  title = {Rene {{Theophile Hyacinthe Laennec}} (1781-1826): {{The Man Behind}} the {{Stethoscope}}},
  shorttitle = {Rene {{Theophile Hyacinthe Laennec}} (1781-1826)},
  author = {Roguin, A.},
  date = {2006-09-01},
  journaltitle = {Clinical Medicine \& Research},
  shortjournal = {Clinical Medicine \& Research},
  volume = {4},
  number = {3},
  pages = {230--235},
  issn = {1539-4182, 1554-6179},
  doi = {10.3121/cmr.4.3.230},
  url = {http://www.clinmedres.org/cgi/doi/10.3121/cmr.4.3.230},
  urldate = {2022-10-10},
  langid = {english},
  keywords = {linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\KPVSRQ94\\Roguin - 2006 - Rene Theophile Hyacinthe Laennec (1781-1826) The .pdf}
}

@book{santosComparativeLatencyAnalysis2021,
  title = {Comparative {{Latency Analysis}} of {{Optical}} and {{Inertial Motion Capture Systems}} for {{Gestural Analysis}} and {{Musical Performance}}},
  author = {Santos, Geise and Wang, Johnty and Brum, Carolina and Wanderley, Marcelo and Fernandes Tavares, Tiago and Rocha, Anderson},
  date = {2021-06-01},
  doi = {10.21428/92fbeb44.51b1c3a1},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\CSZHQ46C\\Santos et al. - 2021 - Comparative Latency Analysis of Optical and Inerti.pdf}
}

@article{schmitzObservationSonifiedMovements2013,
  title = {Observation of Sonified Movements Engages a Basal Ganglia Frontocortical Network},
  author = {Schmitz, Gerd and Mohammadi, Bahram and Hammer, Anke and Heldmann, Marcus and Samii, Amir and Münte, Thomas F. and Effenberg, Alfred O.},
  date = {2013-03-14},
  journaltitle = {BMC Neuroscience},
  shortjournal = {BMC Neurosci},
  volume = {14},
  number = {1},
  pages = {32},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-14-32},
  url = {https://doi.org/10.1186/1471-2202-14-32},
  urldate = {2022-09-01},
  abstract = {Producing sounds by a musical instrument can lead to audiomotor coupling, i.e. the joint activation of the auditory and motor system, even when only one modality is probed. The sonification of otherwise mute movements by sounds based on kinematic parameters of the movement has been shown to improve motor performance and perception of movements.},
  langid = {english},
  keywords = {Brodmann Area,Incongruent Stimulus,Mirror Neuron System,Multisensory Integration,Superior Temporal Sulcus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\J2XMWMQF\\Schmitz et al. - 2013 - Observation of sonified movements engages a basal .pdf}
}

@article{schmitzPERCEPTUALEFFECTSAUDITORY2012,
  title = {{{PERCEPTUAL EFFECTS OF AUDITORY INFORMATION ABOUT OWN AND OTHER MOVEMENTS}}},
  author = {Schmitz, Gerd and Effenberg, Alfred O},
  date = {2012},
  pages = {6},
  abstract = {In sport accurate predictions of other persons’ movements are essential. Former studies have shown that predictions can be enhanced by mapping movements onto sound (sonification) and providing audiovisual feedback [1]. The present study investigated behavioral mechanisms of movement sonification and scrutinized whether effects of own movements and those of other persons can be predicted just by listening to them. Eight athletes heard sonifications of an indoor rower and quantified resulting velocities of a virtual boat. Although boat velocity was not mapped onto sound directly, it explained subjects’ quantifications by regression analysis (R-squared = 0.80) significantly better than the directly sonified amplitude and force parameters. Thus perception of boat velocity might have emerged from those sonifications. Predictions of effects of unknown movements were above chance level and as good as predictions of own movements. Furthermore athletes were able to identify their own technique among others ( d’ = 0.47 ± 0.43). The results confirm large perceptual effects of auditory feedback and - most importantly - suggest that movement sonification can address central motor representations just by listening to it. Therefore not only predictability but also synchronization with other persons’ movements might be supported.},
  langid = {english},
  keywords = {linus,luke,useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\N37CUJK7\\Schmitz and Effenberg - 2012 - PERCEPTUAL EFFECTS OF AUDITORY INFORMATION ABOUT O.pdf}
}

@article{umekSuitabilityStrainGage2017,
  title = {Suitability of {{Strain Gage Sensors}} for {{Integration}} into {{Smart Sport Equipment}}: {{A Golf Club Example}}},
  shorttitle = {Suitability of {{Strain Gage Sensors}} for {{Integration}} into {{Smart Sport Equipment}}},
  author = {Umek, Anton and Zhang, Yuan and Tomažič, Sašo and Kos, Anton},
  date = {2017-04-21},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {17},
  number = {4},
  pages = {916},
  issn = {1424-8220},
  doi = {10.3390/s17040916},
  url = {http://www.mdpi.com/1424-8220/17/4/916},
  urldate = {2022-09-21},
  abstract = {Wearable devices and smart sport equipment are being increasingly used in amateur and professional sports. Smart sport equipment employs various sensors for detecting its state and actions. The correct choice of the most appropriate sensor(s) is of paramount importance for efficient and successful operation of sport equipment. When integrated into the sport equipment, ideal sensors are unobstructive, and do not change the functionality of the equipment. The article focuses on experiments for identification and selection of sensors that are suitable for the integration into a golf club with the final goal of their use in real time biofeedback applications. We tested two orthogonally affixed strain gage (SG) sensors, a 3-axis accelerometer, and a 3-axis gyroscope. The strain gage sensors are calibrated and validated in the laboratory environment by a highly accurate Qualisys Track Manager (QTM) optical tracking system. Field test results show that different types of golf swing and improper movement in early phases of golf swing can be detected with strain gage sensors attached to the shaft of the golf club. Thus they are suitable for biofeedback applications to help golfers to learn repetitive golf swings. It is suggested that the use of strain gage sensors can improve the golf swing technical error detection accuracy and that strain gage sensors alone are enough for basic golf swing analysis. Our final goal is to be able to acquire and analyze as many parameters of a smart golf club in real time during the entire duration of the swing. This would give us the ability to design mobile and cloud biofeedback applications with terminal or concurrent feedback that will enable us to speed-up motor skill learning in golf.},
  langid = {english},
  keywords = {latency},
  file = {C\:\\Users\\webma\\Zotero\\storage\\ENHX8XEA\\Umek et al. - 2017 - Suitability of Strain Gage Sensors for Integration.pdf}
}

@article{vinkenAuditoryCodingHuman2013,
  title = {Auditory {{Coding}} of {{Human Movement Kinematics}}},
  author = {Vinken, Pia M. and Kröger, Daniela and Fehse, Ursula and Schmitz, Gerd and Brock, Heike and Effenberg, Alfred~O.},
  date = {2013},
  journaltitle = {Multisensory Research},
  shortjournal = {Multisens Res},
  volume = {26},
  number = {6},
  pages = {533--552},
  issn = {2213-4794, 2213-4808},
  doi = {10.1163/22134808-00002435},
  url = {https://brill.com/view/journals/msr/26/6/article-p533_2.xml},
  urldate = {2022-09-01},
  abstract = {Although visual perception is dominant on motor perception, control and learning, auditory information can enhance and modulate perceptual as well as motor processes in a multifaceted manner. During last decades new methods of auditory augmentation had been developed with movement sonification as one of the most recent approaches expanding auditory movement information also to usually mute phases of movement. Despite general evidence on the effectiveness of movement sonification in different fields of applied research there is nearly no empirical proof on how sonification of gross motor human movement should be configured to achieve information rich sound sequences. Such lack of empirical proof is given for (a) the selection of suitable movement features as well as for (b) effective kinetic–acoustical mapping patterns and for (c) the number of regarded dimensions of sonification. In this study we explore the informational content of artificial acoustical kinematics in terms of a kinematic movement sonification using an intermodal discrimination paradigm. In a repeated measure design we analysed discrimination rates of six everyday upper limb actions to evaluate the effectiveness of seven different kinds of kinematic–acoustical mappings as well as short term learning effects. The kinematics of the upper limb actions were calculated based on inertial motion sensor data and transformed into seven different sonifications. Sound sequences were randomly presented to participants and discrimination rates as well as confidence of choice were analysed. Data indicate an instantaneous comprehensibility of the artificial movement acoustics as well as short term learning effects. No differences between different dimensional encodings became evident thus indicating a high efficiency for intermodal pattern discrimination for the acoustically coded velocity distribution of the actions. Taken together movement information related to continuous kinematic parameters can be transformed into the auditory domain. Additionally, pattern based action discrimination is obviously not restricted to the visual modality. Artificial acoustical kinematics might be used to supplement and/or substitute visual motion perception in sports and motor rehabilitation.},
  langid = {english},
  keywords = {linus,luke,useful},
  file = {C\:\\Users\\webma\\Zotero\\storage\\DII8GFIJ\\Vinken et al. - 2013 - Auditory Coding of Human Movement Kinematics.pdf}
}

@article{walkerMUSICALSOUNDSCAPESACCESSIBLE2007,
  title = {{{MUSICAL SOUNDSCAPES FOR AN ACCESSIBLE AQUARIUM}}: {{BRINGING DYNAMIC EXHIBITS TO THE VISUALLY IMPAIRED}}},
  author = {Walker, Bruce N and Kim, Jonathan and Pendse, Anandi},
  date = {2007},
  journaltitle = {Proceedings of the 2007 International Computer Music Conference, ICMC 2007, Copenhagen, Denmark, August 27-31, 2007.},
  pages = {8},
  abstract = {In an effort to make an aquarium, zoo, or other dynamic “informal learning environment” more accessible to the visually impaired, we track the fish (and other creatures) with computer vision, then use the movement data to create meaningful and aesthetic music. Here we present four new classes of “soundscapes”, which demonstrate a range of data-to-music mapping approaches. This follows on the initial prototype work, discussed previously. A systematic exploration of the possible composition and design space is leading to music that communicates the dynamic aspects of the exhibit (e.g., how many fish, what kinds, where they are, what they are doing), as well as conveying the emotional content (e.g., amazement and wonder at the massive whale shark gliding by). Informal evaluations have been very successful; formal evaluations are ongoing.},
  langid = {english},
  keywords = {linus},
  file = {C\:\\Users\\webma\\Zotero\\storage\\GE5MJGB4\\Walker et al. - MUSICAL SOUNDSCAPES FOR AN ACCESSIBLE AQUARIUM BR.pdf}
}

@article{zammSynchronizingMIDIWireless2019,
  title = {Synchronizing {{MIDI}} and Wireless {{EEG}} Measurements during Natural Piano Performance},
  author = {Zamm, Anna and Palmer, Caroline and Bauer, Anna-Katharina R. and Bleichner, Martin G. and Demos, Alexander P. and Debener, Stefan},
  date = {2019-08},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  volume = {1716},
  pages = {27--38},
  issn = {00068993},
  doi = {10.1016/j.brainres.2017.07.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899317302913},
  urldate = {2022-09-14},
  langid = {english},
  keywords = {methods,useful,useful-structure},
  file = {C\:\\Users\\webma\\Zotero\\storage\\DD2MAP6N\\Zamm et al. - 2019 - Synchronizing MIDI and wireless EEG measurements d.pdf}
}


