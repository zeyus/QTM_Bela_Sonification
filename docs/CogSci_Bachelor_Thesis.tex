\documentclass[10pt,a4paper,onecolumn]{article}
\usepackage{marginnote}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{authblk,etoolbox}
\usepackage{titlesec}
\usepackage{calc}
\usepackage{tikz}
\usepackage{hyperref}
\hypersetup{colorlinks,
            urlcolor=[rgb]{0.0, 0.5, 1.0},
            linkcolor=[rgb]{0.0, 0.5, 1.0}}
\usepackage{caption}
\usepackage{tcolorbox}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{seqsplit}
%\usepackage{changepage}
%\usepackage{multicol}
% \usepackage{fixltx2e} % provides \textsubscript
\usepackage[backend=biber,style=apa]{biblatex}

\addbibresource{master.bib}
\addbibresource{packages.bib}

% --- Page layout -------------------------------------------------------------
\usepackage[top=3.5cm, bottom=3cm, right=1.5cm, left=1.5cm,
            headheight=2.2cm, reversemp, marginparwidth=0cm, marginparsep=0cm]{geometry}

% --- Default font ------------------------------------------------------------
% \renewcommand\familydefault{\sfdefault}

% --- Style -------------------------------------------------------------------
\renewcommand{\bibfont}{\small \sffamily}
\renewcommand{\captionfont}{\small\sffamily}
\renewcommand{\captionlabelfont}{\bfseries}

% --- Section/SubSection/SubSubSection ----------------------------------------
\titleformat{\section}
  {\normalfont\sffamily\Large\bfseries}
  {}{0pt}{}
\titleformat{\subsection}
  {\normalfont\sffamily\large\bfseries}
  {}{0pt}{}
\titleformat{\subsubsection}
  {\normalfont\sffamily\bfseries}
  {}{0pt}{}
\titleformat*{\paragraph}
  {\sffamily\normalsize}


% --- Header / Footer ---------------------------------------------------------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
%\renewcommand{\headrulewidth}{0.50pt}
\renewcommand{\headrulewidth}{0pt}

\fancyhead[L]{}
\fancyhead[C]{}
\fancyhead[R]{}
\renewcommand{\footrulewidth}{0.25pt}

\fancyfoot[L]{\footnotesize{\sffamily Backström, L. and Ring, L., (2023). Harmony in Motion: Real-time Sonification Strategies for Joint Action Research., .}}


\fancyfoot[R]{\sffamily \thepage}
\makeatletter
\let\ps@plain\ps@fancy
\fancyheadoffset[L]{0cm}
\fancyfootoffset[L]{0cm}

% --- Macros ---------

\definecolor{linky}{rgb}{0.0, 0.5, 1.0}

\newtcolorbox{repobox}
   {colback=red, colframe=red!75!black,
     boxrule=0.5pt, arc=2pt, left=6pt, right=6pt, top=3pt, bottom=3pt}

\newcommand{\ExternalLink}{%
   \tikz[x=1.2ex, y=1.2ex, baseline=-0.05ex]{%
       \begin{scope}[x=1ex, y=1ex]
           \clip (-0.1,-0.1)
               --++ (-0, 1.2)
               --++ (0.6, 0)
               --++ (0, -0.6)
               --++ (0.6, 0)
               --++ (0, -1);
           \path[draw,
               line width = 0.5,
               rounded corners=0.5]
               (0,0) rectangle (1,1);
       \end{scope}
       \path[draw, line width = 0.5] (0.5, 0.5)
           -- (1, 1);
       \path[draw, line width = 0.5] (0.6, 1)
           -- (1, 1) -- (1, 0.6);
       }
   }

% --- Title / Authors ---------------------------------------------------------
% patch \maketitle so that it doesn't center
\patchcmd{\@maketitle}{center}{flushleft}{}{}
\patchcmd{\@maketitle}{center}{flushleft}{}{}
% patch \maketitle so that the font size for the title is normal
\patchcmd{\@maketitle}{\LARGE}{\LARGE\sffamily}{}{}
% patch the patch by authblk so that the author block is flush left
\def\maketitle{{%
  \renewenvironment{tabular}[2][]
    {\begin{flushleft}}
    {\end{flushleft}}
  \AB@maketitle}}
\makeatletter
\renewcommand\AB@affilsepx{ \protect\Affilfont}
%\renewcommand\AB@affilnote[1]{{\bfseries #1}\hspace{2pt}}
\renewcommand\AB@affilnote[1]{{\bfseries #1}\hspace{3pt}}
\makeatother
\renewcommand\Authfont{\sffamily\bfseries}
\renewcommand\Affilfont{\sffamily\small\mdseries}
\setlength{\affilsep}{1em}


\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}

\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}

\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Harmony in Motion: Real-time Sonification Strategies for Joint Action Research},
            pdfkeywords={Sonification; Motion Capture; Realtime; Processing},
            pdfborder={0 0 0},
            }
\urlstyle{same}  % don't use monospace font for urls
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% From pandoc table feature
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}



\title{Harmony in Motion: Real-time Sonification Strategies for Joint Action Research}

        \author[1]{Linus Backström}
          \author[1]{Luke Ring}
    
      \affil[1]{Aarhus University}
  \date{\vspace{-5ex}}
\begin{document}
\newgeometry{includemp, reversemp, left=1.0cm, marginparwidth=4.5cm, marginparsep=0.5cm}
    \maketitle
      \begin{abstract}
  Placeholder: For any time-sensitive task making use of auditory feedback, having low latency is vital to ensure the sonification feels connected to the action, rather than disjointed\ldots{}
  \end{abstract}
  
  \marginpar{
    \sffamily\small

    \vspace{2mm}

    {\bfseries Software}
    \begin{itemize}
      \setlength\itemsep{0em}
      \item \href{https://github.com/zeyus/QTM\_Bela\_Sonification}{\color{linky}{Repository}} \ExternalLink
    \end{itemize}

    \vspace{2mm}

    {\bfseries Submitted:} December 20, 2022

    \vspace{2mm}
    {\bfseries License}\\
    Authors of papers retain copyright and release the work under a MIT Licence (\href{https://github.com/zeyus/QTM\_Bela\_Sonification/blob/main/LICENSE.md}{\color{linky}{MIT}}).
  }
\restoregeometry
{
\setcounter{tocdepth}{3}
\tableofcontents
}
\twocolumn

% This will be displayed full-width
\hypertarget{harmony-in-motion-real-time-sonification-strategies-for-joint-action-research}{%
\section{Harmony in Motion: Real-time Sonification Strategies for Joint Action Research}\label{harmony-in-motion-real-time-sonification-strategies-for-joint-action-research}}

Joint action tasks form an integral part of the everyday life of humans and many other species {[}ref{]},
and the mechanisms underlying this cooperative ability to work together towards a common goal have been
the subject of an increasing number of publications {[}ref, scholar search?{]}. Many types of cooperation
involve auditory perception as a key aspect, either as the focus of the task -- as is the case for
musicians in a band -- or as a component that can be leveraged for increasing synchronization,
for example with a steady beat to set the pace for rowers. For any time-sensitive task making
use of auditory feedback, having low latency is vital to ensure the sonification feels connected
to the action, rather than disjointed {[}ref{]}. While there has been research into the effects of
sonification on joint action {[}ref{]}, this thesis presents a flexible low latency sonification
framework that uses real-time positional data for joint action research. This framework has been
implemented in a pilot study to investigate the utility of a novel joint action synchrony paradigm
that puts the focus of the sonification strategy as a core aspect of study design in this field. By
comparing subject synchronization during experiments employing task-oriented or synchronization-oriented
strategies, we have attempted to show differences that highlight the importance of strategy selection
for sonification and provide a pathway for further investigation.

\hypertarget{background}{%
\section{Background}\label{background}}

\hypertarget{joint-action}{%
\subsection{Joint Action}\label{joint-action}}

Joint actions, where two or more people synchronize their actions in pursuit of a shared goal \autocite{knoblichPsychologicalResearchJoint2011}, are a regular part of human behaviour. Examples include handshakes, conversations, musical performances\ldots{} dancing

What are the cognitive processes involved in joint action? Recent theory suggests that the main cognitive processes involved include representations, action monitoring and action prediction \autocite{vesperMinimalArchitectureJoint2010,loehrMonitoringIndividualJoint2013,sebanzJointActionBodies2006}.

\hypertarget{representations}{%
\subsubsection{Representations}\label{representations}}

According to the minimal architecture for joint action proposed by \textcite{vesperMinimalArchitectureJoint2010}, an agent involved in joint action must, at a minimum, have a representation of their own task and the shared goal. While it is not required, it is usually helpful to also represent the other's task, as it allows for more precise predictions of what the other will do next \autocite{boltSensoryAttenuationAuditory2021}. As an example, consider two singers performing a duet together. Each singer must fully know their own part, while also representing the shared goal of synchronized singing. Although these two main representations can be sufficient for performing a duet, professional singers typically familiarize themselves with their singing partner's part in addition to their own, as it allows for a more polished and cohesive musical performance.

Further insight into the role of representations in joint action comes from an EEG study by \textcite{kourtisPredictiveRepresentationOther2012}, which found that partners represented each other's actions in advance when passing an object, and doing so facilitated coordination. Having these shared representations of actions and their underlying goals allows individuals to establish a procedural common ground for joint action without needing to rely on symbolic communication \autocite{sebanzJointActionBodies2006}.

\hypertarget{action-monitoring}{%
\subsubsection{Action Monitoring}\label{action-monitoring}}

Monitoring processes are used to assess the extent to which a task or goal is being accomplished and whether actions are proceeding as intended (Botvinick et al., 2001). In terms of assessing task and goal progress, three things can be monitored: one's own task, the other's task, and the shared goal. As with representations, one must at least monitor the progress of one's own task and the shared goal. It is not strictly necessary to monitor the other's task, and it depends on the type of joint action that is performed. Nevertheless, it is likely true that monitoring what one's partner is doing will improve joint action performance -- especially for tasks that require precise synchronization \autocite{vesperMinimalArchitectureJoint2010}.

\hypertarget{action-predicting}{%
\subsubsection{Action Predicting}\label{action-predicting}}

In order to coordinate effectively, it is often necessary to make predictions about future events. This prediction process is achieved through motor simulation, which uses internal models to determine the sensory consequences of actions as well as their effect on the environment (Vesper et al., 2010).

\hypertarget{integrating-predicted-outcomes-of-ones-own-and-others-actions}{%
\subsubsection{Integrating Predicted Outcomes of One's Own and Others' Actions}\label{integrating-predicted-outcomes-of-ones-own-and-others-actions}}

The crucial final feature of joint action is the manner in which individuals adapt their own actions to those of others in time and space. The question concerns the decision-making process that occurs after a prediction of another person's action is made, i.e.~choosing a suitable complementary action and performing it at the appropriate time. In order to avoid constantly being one step behind during joint action, interacting partners cannot simply respond to observed actions, but must rather plan their own actions in relation to what they predict their partner will do \autocite{sebanzJointActionBodies2006}. A study by Knoblich and Jordan {[}Sebanz 43-34{]} using a joint action tracking paradigm found that partners planned their actions based on a prediction of what the joint effect of both their own and their partner's actions would be.

Some of the questions that have been investigated in recent joint action research concern the aforementioned processes (representations, action monitoring and action predicting) and how they relate to agency (self vs other) and outcome (individual vs joint). Researchers have studied whether agents involved in joint action represent both their own task and their partner's task (citation), whether they monitor individual outcomes or joint outcomes, and

\hypertarget{current-study}{%
\subsubsection{Current Study}\label{current-study}}

The current study had participants perform a joint action task under three different conditions, where the sensory consequences were manipulated through real-time sonification of movement.

The current study focuses on what happens during learning of joint action. Through movement sonification, we can enhance attention towards specific features of joint action. Is learning optimized when focusing on self-other representations or joint outcome representations?

While we could not completely separate the conditions, our sonifications focus attention towards either self-other monitoring, or joint outcome.

\hypertarget{sonification}{%
\subsection{Sonification}\label{sonification}}

Sonification is defined as the use of nonspeech audio to convey information. More specifically, sonification is the transformation of data relations into perceived relations in an acoustic signal for the purposes of facilitating communication or interpretation \autocite[p.~4]{kramerSonificationReportStatus1999}.

While concepts around sonification and audification were not formalized until around the year 1992 when the first International Conference on Auditory Display (ICAD) was held \autocite{dubusSonificationPhysicalQuantities2011}, practical examples of sonification can be found throughout history. Water clocks in ancient Greece and medieval China were sometimes constructed to produce sounds and thereby provide auditory information about the passage of time \autocite{dubusSonificationPhysicalQuantities2011}. The stethoscope, which is used for listening to sounds made by the heart and lungs as well as other internal sounds of the body, was invented in 1816 by French physician and amateur musician Rene Laënnec \autocite{roguinReneTheophileHyacinthe2006}. The Geiger counter developed in 1928 provides perhaps the most characteristic example of sonification through its function of sonifying levels of radiation. The device detects ionizing radiation and translates it into audible clicks, where a faster tempo signifies a higher level of radiation. \textcite{dubusSonificationPhysicalQuantities2011} describe the value of the Geiger counter as ``transposing a physical quantity which is essentially non-visual and pictured in everyone's imagination as very important because life-threatening, to the auditory modality through clicks with a varying pulse''.

\hypertarget{what-is-sonification-useful-for}{%
\subsubsection{What is sonification useful for?}\label{what-is-sonification-useful-for}}

Making sense of large amounts of data, and utilizing modern powerful media technologies. ``Sonification research is well positioned to provide technology to assist scientists in comprehending the information and data-rich world of today and of the future.''. ``The wide availability of audio technology (e.g., in multimedia computers) makes auditory data representation a viable option for large numbers of users. Thus, there exists today a synergism between the widespread need for new data comprehension methods and readily available technology that, with proper support and funding, can lead to a large number of users reaping the benefits conferred by the development of scientific sonification.'' \autocite{kramerSonificationReportStatus1999}

\hypertarget{movement-sonification}{%
\subsubsection{Movement sonification}\label{movement-sonification}}

``Approaches within the discipline of Sport Science reflect the whole range from fundamental research with high internal validity to applied research with high ecological validity. Applied research plays an important role for the development of new, more effective intervention methods. Assuming that more senses are more powerful in perceiving gross motor patterns it should be supportive to create and convey more acoustic movement information. For multisensory integration benefits, additional auditory movement information has to correspond to the structure of a perceptual feature stream of another modality (visual, kinesthetic, tactile). For such an acoustic enhancement of motor perception Effenberg (1996, 2004, 2005) has established the concept of `movement sonification', adapting the sonification approach of the early 1990s to the kinematics and dynamics of human motor behavior.'' One of the movement parameters that can be used is ``kinematic parameters representing the spatiotemporal features of a pose or a movement pattern.'' ``The question whether dynamic or kinematic movement parameters should be chosen for movement sonification should be answered under consideration of the sensory modality or modalities with which bi- or multimodal convergence should be achieved: If visual motion perception is the reference, movement sonification should be based on kinematic parameters.'' \autocite{gerdschmitzSoundJoinedActions2017}

\begin{quote}
``Subjects are able to perceive differences in swimming stroke frequency more accurately when visualizations of a swimmer are complemented with a kinematic sonification.'' \autocite{gerdschmitzSoundJoinedActions2017}
\end{quote}

\hypertarget{auditory-perception}{%
\subsubsection{Auditory perception}\label{auditory-perception}}

Research into auditory perception indicates two basic features of auditory perception that provide good arguments for representing data as sound. First, auditory perception is especially useful for detecting temporal characteristics, i.e.~variations in sound over time. {[}find study that said we're better at detecting rhythm aurally vs visually?{]} Sonification can thus be useful for monitoring or understanding complex temporal data. Second, our sense of hearing does not require us to be oriented towards the sound source. Unlike visual perception, which allows us to perceive approximately 180 degrees of our environment in front of us while we remain blind to the other 180 degrees behind us, auditory perception allows perception of 360 degrees. This makes auditory signals particularly useful for situations where our visual system is occupied with another task and we cannot afford to look around constantly, such as monitoring and alarm applications. \autocite{kramerSonificationReportStatus1999}

Other benefits of auditory perception that speak for sonification: parallel listening (ability to monitor and process multiple auditory data sets), rapid detection (especially in high-stress environments), affective response (ease of learning and high engagement qualities) and auditory gestalt formation (discerning relationships or trends in data streams) \autocite{kramerSonificationReportStatus1999}

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{pilot-experiment}{%
\subsection{Pilot Experiment}\label{pilot-experiment}}

A pilot experiment was conducted that required subjects to\ldots{}

Describe exp setup and subject demographics etc. See Figure \ref{fig:track-setup}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/track_dimensions} 

}

\caption{Experimental track setup}\label{fig:track-setup}
\end{figure}

\hypertarget{hardware-and-software}{%
\subsection{Hardware and Software}\label{hardware-and-software}}

\hypertarget{motion-capture}{%
\subsubsection{Motion Capture}\label{motion-capture}}

Motion capture data were collected using a 9 (8 Qualisys Miqus M3 marker and 1 Qualisys Miqus Video) camera system connected to a Qualisys Camera Sync Unit.
Marker data were acquired at a sampling rate of 300 Hz and video data were acquired at a sampling rate of 25 Hz. Qualisys Track Manager version 2022.2 (build 7700) software was used to collect and process the data with real-time 3D tracking data output.

\hypertarget{markers}{%
\paragraph{Markers}\label{markers}}

For the experimental set up, one passive marker was placed on each car, and additional passive markers were placed at the start and end of each track, as well as one passive marker on each corner of the surface the tracke was mounted on (see Fig X).
These additional markers provided reference points for 3D orientation of the track and the cars across trials in case of accidental table movement.

\hypertarget{sonification-1}{%
\subsubsection{Sonification}\label{sonification-1}}

\hypertarget{hardware}{%
\paragraph{Hardware}\label{hardware}}

Motion capture data were sent via UDP packets over USB networking to a Bela Mini computer running version 0.3.8g running a custom C++ program\footnote{Source, data and analysis are available at \url{https://github.com/zeyus/QTM_Bela_Sonification}}. The main program loop
was configured to execute every 32 samples, with an output sample rate of \ensuremath{4.41\times 10^{4}} Hz for 2 audio channels. The stereo audio output was split into
two channels which were connected to a pair of headphones for each subject.

\hypertarget{software}{%
\paragraph{Software}\label{software}}

The main program used the Bela platform from 10 August 2022\footnote{\url{https://github.com/BelaPlatform/Bela/commit/42bbf18c3710ed82cdf24b09ab72ac2239bf148e}}.

\hypertarget{experimental-design}{%
\subsection{Experimental Design}\label{experimental-design}}

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/mt_trajectories_task_sonification} 

}

\caption{Plot of trajectories from subject pairs in the Task-oriented sonification condition}\label{fig:mt-trajectories}
\end{figure}

Test cross-ref, see figure \ref{fig:mt-trajectories}

\hypertarget{results}{%
\section{Results}\label{results}}

\onecolumn

\printbibliography[title=References,heading=bibintoc]

\end{document}
